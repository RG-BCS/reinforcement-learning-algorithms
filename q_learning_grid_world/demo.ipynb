{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning in GridWorld\n",
        "\n",
        "This notebook demonstrates how to implement **Q-Learning** in a custom GridWorld environment using Python. The agent learns to navigate from any valid cell to a goal state while avoiding obstacles.\n",
        "\n",
        "Key Concepts:\n",
        "- **Q-Learning**: Off-policy Temporal Difference control algorithm.\n",
        "- **ε-greedy exploration**: Balances exploration and exploitation.\n",
        "- **GridWorld**: A simple environment with a reward structure.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "VQThaYJOv-vW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from utils import (\n",
        "    plot_arrows_MDP_QL,\n",
        "    plot_grid_world_MDP_QL,\n",
        "    simulate_policy_MDP_QL,\n",
        "    print_value_grid\n",
        ")\n"
      ],
      "metadata": {
        "id": "Gj1tIwdEwAsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the GridWorld Environment\n",
        "\n",
        "- 5x6 grid.\n",
        "- Negative step penalty, goal reward.\n",
        "- Obstacles at specific coordinates.\n"
      ],
      "metadata": {
        "id": "GhEkg6wLwJlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid_world = [5, 6]\n",
        "GOAL_REWARD = 10.0\n",
        "STEP_PENALTY = -1.0\n",
        "OBSTACLES = [(1, 4), (2, 3), (3, 4)]\n",
        "GOAL_STATE = (2, 4)\n",
        "\n",
        "actions = np.array(['up', 'down', 'left', 'right'])\n",
        "action_moves = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
        "action_to_index = {act: i for i, act in enumerate(actions)}\n",
        "\n",
        "states = [(j, i) for j in range(grid_world[0]) for i in range(grid_world[1]) if (j, i) not in OBSTACLES]\n",
        "state_to_index = {s: idx for idx, s in enumerate(states)}\n",
        "GOAL_INDEX = state_to_index[GOAL_STATE]\n",
        "\n",
        "Q_values = np.zeros((len(states), len(actions)))\n",
        "\n",
        "def validate_policy_result(current_state, move):\n",
        "    next_state = current_state[0] + move[0], current_state[1] + move[1]\n",
        "    if next_state not in OBSTACLES and (0 <= next_state[0] < grid_world[0]) and (0 <= next_state[1] < grid_world[1]):\n",
        "        return next_state, True\n",
        "    return current_state, False\n"
      ],
      "metadata": {
        "id": "Xf7Elxu3wP0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning Algorithm\n",
        "\n",
        "The agent runs multiple episodes to learn the optimal policy using the Bellman update.\n"
      ],
      "metadata": {
        "id": "D6riteFSwXXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episodes_update_Q(Q_values, valid_starts, n_episodes, learning_rate=0.1,\n",
        "                          gamma=1.0, max_steps=100, epsilon=0.1, epsilon_decay=0.995):\n",
        "    for episode in range(n_episodes):\n",
        "        s = valid_starts[np.random.choice(len(valid_starts))]\n",
        "        steps = 0\n",
        "        while s != GOAL_STATE and steps < max_steps:\n",
        "            if np.random.rand() < epsilon:\n",
        "                action_name = np.random.choice(actions)\n",
        "            else:\n",
        "                action_name = actions[np.argmax(Q_values[state_to_index[s]])]\n",
        "\n",
        "            move = action_moves[action_name]\n",
        "            sp, valid = validate_policy_result(s, move)\n",
        "            action_idx = action_to_index[action_name]\n",
        "            s_idx = state_to_index[s]\n",
        "            sp_idx = state_to_index[sp]\n",
        "            r = GOAL_REWARD if sp == GOAL_STATE else STEP_PENALTY\n",
        "\n",
        "            Q_values[s_idx, action_idx] += learning_rate * (r + gamma * np.max(Q_values[sp_idx]) - Q_values[s_idx, action_idx])\n",
        "            s = sp\n",
        "            steps += 1\n",
        "        epsilon = max(0.01, epsilon * epsilon_decay)\n"
      ],
      "metadata": {
        "id": "ghobFlKkwZ4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Agent\n"
      ],
      "metadata": {
        "id": "KtZI-qJ0wf2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_starts = [s for s in states if s != GOAL_STATE]\n",
        "run_episodes_update_Q(Q_values, valid_starts, n_episodes=2000)\n"
      ],
      "metadata": {
        "id": "j5CQufdbwiBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the Learned Policy\n",
        "\n",
        "We print the Q-table and overlay arrows representing the optimal policy.\n"
      ],
      "metadata": {
        "id": "oxqHY4CVwlvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_value_grid(Q_values)\n",
        "plot_arrows_MDP_QL(Q_values, grid_world, actions, states, goal_state=GOAL_STATE,\n",
        "                   OBSTACLES=OBSTACLES, state_to_index=state_to_index)\n"
      ],
      "metadata": {
        "id": "hlJ6Xh39wo8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulate Optimal Path from a Start State\n"
      ],
      "metadata": {
        "id": "txXvTgkgwtJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "START_STATE = (4, 0)\n",
        "START_INDEX = state_to_index[START_STATE]\n",
        "\n",
        "path = simulate_policy_MDP_QL(Q_values, states, actions, start_index=START_INDEX,\n",
        "                               goal_index=GOAL_INDEX, grid_world=grid_world, OBSTACLES=OBSTACLES, action_moves=action_moves)\n",
        "\n",
        "plot_arrows_MDP_QL(Q_values, grid_world, actions, states, goal_state=GOAL_STATE, OBSTACLES=OBSTACLES,\n",
        "                   path=path, state_to_index=state_to_index)\n"
      ],
      "metadata": {
        "id": "ykZrCVo7wxG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Visualization: GridWorld with Path\n"
      ],
      "metadata": {
        "id": "KUy6Wobzw1Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_grid_world_MDP_QL(states, grid_size=grid_world, obstacles=OBSTACLES,\n",
        "                       goal=GOAL_STATE, path=path, Q_values=Q_values, actions=actions)\n"
      ],
      "metadata": {
        "id": "qOrMNtcMw3I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Summary: Q-Learning in GridWorld**\n",
        "\n",
        "*This project demonstrates how to implement Q-Learning, an off-policy reinforcement learning algorithm, in a custom-built GridWorld environment. The agent learns to navigate through a grid, avoid obstacles, and reach a designated goal using the ε-greedy strategy for exploration.*\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "Environment: 5x6 GridWorld with impassable obstacles and a rewarding goal.\n",
        "\n",
        "**Learning Algorithm:**\n",
        "\n",
        "Q-Learning with dynamic ε-decay and TD updates.\n",
        "\n",
        "**Visualization:**\n",
        "\n",
        "Arrow-based policy map\n",
        "\n",
        "Grid overlay with obstacles, start, and goal\n",
        "\n",
        "Simulated path from a chosen start state\n",
        "\n",
        "**Reinforcement Learning Concepts Covered:**\n",
        "\n",
        "State-action value function (Q-values)\n",
        "\n",
        "Bellman Equation update\n",
        "\n",
        "ε-greedy policy for exploration vs. exploitation\n",
        "\n",
        "Off-policy learning\n",
        "\n",
        "**Outcome:**\n",
        "\n",
        "After training, the agent reliably learns an optimal policy that maximizes cumulative rewards, visualized through intuitive grid-based plots."
      ],
      "metadata": {
        "id": "0DTEGbs6xQaf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qr1UK4cFxlW7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}