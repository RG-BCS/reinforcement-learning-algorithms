{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# REINFORCE Algorithm (No Baseline) — CartPole-v1\n",
        "\n",
        "This notebook demonstrates a stochastic policy gradient method — the **REINFORCE algorithm** — implemented from scratch using TensorFlow and applied to the classic `CartPole-v1` environment.\n",
        "\n",
        "### Key Characteristics:\n",
        "- No baseline used (pure Monte Carlo returns)\n",
        "- Stochastic policy output from a neural network\n",
        "- Trained using log-likelihood gradient scaling\n"
      ],
      "metadata": {
        "id": "38P8DzA29uAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from reinforce_policy import (build_policy_network,evaluate_policy,train_policy_model)\n",
        "\n",
        "np.random.seed(65)\n",
        "tf.random.set_seed(65)"
      ],
      "metadata": {
        "id": "rQ8Q0btQ9tCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try a Handcrafted Policy (Angle Heuristic)\n",
        "\n",
        "Let's see how well a basic deterministic policy performs before training anything.\n"
      ],
      "metadata": {
        "id": "fy2nE26W-DnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "def angle_policy(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1\n",
        "\n",
        "rewards = []\n",
        "for episode in range(500):\n",
        "    obs = env.reset()\n",
        "    total = 0\n",
        "    for _ in range(200):\n",
        "        action = angle_policy(obs)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total += reward\n",
        "        if done:\n",
        "            break\n",
        "    rewards.append(total)\n",
        "\n",
        "rewards = np.array(rewards)\n",
        "print(f\"Mean reward: {rewards.mean():.2f}\")\n",
        "print(f\"Max reward: {rewards.max():.2f}\")\n",
        "print(f\"Min reward: {rewards.min():.2f}\")"
      ],
      "metadata": {
        "id": "sY-Epw0g-GuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train REINFORCE Policy (Stochastic)\n",
        "\n",
        "We'll train a shallow neural network using the REINFORCE algorithm with Monte Carlo returns and a log-likelihood loss."
      ],
      "metadata": {
        "id": "iykmOy7u-MoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the stochastic policy model\n",
        "model = build_policy_network()\n",
        "\n",
        "# Optimizer with fixed learning rate\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "# Evaluate before training\n",
        "pretrain_reward = evaluate_policy(env, model)\n",
        "print(f\"Reward before training: {pretrain_reward}\")\n",
        "\n",
        "# Train using REINFORCE algorithm\n",
        "reward_history = train_policy_model(env_name=\"CartPole-v1\",model=model,\n",
        "                                    optimizer=optimizer,episodes=500,gamma=0.99)\n"
      ],
      "metadata": {
        "id": "lhlN9dN7-WjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Progress\n"
      ],
      "metadata": {
        "id": "686MbHzf-beQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(reward_history)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"REINFORCE Training Progress\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N1qhJWmO-zKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate after training"
      ],
      "metadata": {
        "id": "QvbKexUW_JLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate after training\n",
        "posttrain_reward = evaluate_policy(env, model)\n",
        "print(f\"Reward after training: {posttrain_reward}\")\n",
        "\n",
        "# Average over multiple runs\n",
        "eval_runs = [evaluate_policy(env, model) for _ in range(20)]\n",
        "print(f\"Mean reward over 20 eval runs: {np.mean(eval_runs):.2f}\")\n"
      ],
      "metadata": {
        "id": "ZKkMXraV-6Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "- The REINFORCE algorithm improved our policy significantly.\n",
        "- Stochastic sampling from the model’s output was key to exploration.\n",
        "- The reward steadily increased toward the 500-step max in CartPole.\n",
        "\n",
        "Next steps:\n",
        "- Add a **baseline** to reduce variance\n",
        "- Explore **actor-critic methods**\n"
      ],
      "metadata": {
        "id": "MBZPueBx_E6r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2xMlLxau_HSE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}