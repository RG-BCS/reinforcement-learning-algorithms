# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

"""
reinforce_policy.py

Implementation of the REINFORCE algorithm (policy gradient, no baseline)
on CartPole-v1 using a stochastic policy modeled by a neural network.
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
import gym


def build_policy_network(input_dim=4, hidden_units=5):
    """
    Builds a stochastic policy network.

    The output is a single probability between 0 and 1, representing the
    chance of taking action 'left'. Final action is sampled using this prob.
    """
    return keras.Sequential([
        keras.layers.InputLayer(input_shape=(input_dim,)),
        keras.layers.Dense(hidden_units, activation="elu"),
        keras.layers.Dense(1, activation="sigmoid")  # Outputs P(left)
    ])


def evaluate_policy(env, model, max_steps=500):
    """
    Evaluate the current policy model in the environment.

    The policy is stochastic — action is sampled from the network's output.
    """
    obs = env.reset()
    total_reward = 0
    done = False
    steps = 0

    while not done and steps < max_steps:
        p_left = model(obs.reshape(1, -1), training=False)[0, 0].numpy()
        action = int(np.random.rand() > p_left)  # ← stochastic decision
        obs, reward, done, _ = env.step(action)
        total_reward += reward
        steps += 1

    return total_reward


def play_episode(env, model, gamma=0.99, epsilon=1e-9):
    """
    Plays one episode and collects states, actions, and normalized returns.

    Uses Monte Carlo estimate of return (no baseline).
    Policy is stochastic — sampled using model's output probability.
    """
    states, actions, rewards = [], [], []
    obs = env.reset()
    done = False

    while not done:
        states.append(obs)
        p_left = model(obs.reshape(1, -1), training=False)[0, 0].numpy()
        action = int(np.random.rand() > p_left)  # ← stochastic sampling from policy
        obs, reward, done, _ = env.step(action)
        actions.append(action)
        rewards.append(reward)

    # Compute discounted return (G_t) at each timestep
    G = 0
    discounted_rewards = []
    for r in reversed(rewards):
        G = r + gamma * G
        discounted_rewards.insert(0, G)

    # Normalize rewards (zero mean, unit variance) for stability
    discounted_rewards = np.array(discounted_rewards)
    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + epsilon)

    return np.array(states), np.array(actions), discounted_rewards, rewards


def train_policy_model(env_name, model, optimizer, episodes=500, gamma=0.99, epsilon=1e-9):
    """
    Trains the policy network using REINFORCE algorithm (no baseline).

    Optimizes the log-prob of actions multiplied by the discounted return.
    """
    env = gym.make(env_name)
    env.seed(42)
    env.action_space.seed(42)

    reward_history = []

    for ep in range(episodes):
        with tf.GradientTape() as tape:
            states, actions, disc_rewards, raw_rewards = play_episode(env, model, gamma, epsilon)

            states = tf.convert_to_tensor(states, dtype=tf.float32)
            actions = tf.convert_to_tensor(actions, dtype=tf.float32)
            disc_rewards = tf.convert_to_tensor(disc_rewards, dtype=tf.float32)

            # Forward pass to get action probabilities
            probs = tf.reshape(model(states), (-1,))
            action_probs = tf.where(actions == 0, probs, 1 - probs)  # stochastic prob of selected action
            action_probs = tf.clip_by_value(action_probs, 1e-8, 1.0)

            # Log-likelihood loss scaled by return
            log_probs = tf.math.log(action_probs + epsilon)
            loss = -tf.reduce_mean(log_probs * disc_rewards)

        # Backpropagation
        grads = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
        reward_history.append(np.sum(raw_rewards))

        if ep % 50 == 0:
            print(f"Episode {ep}: reward = {np.sum(raw_rewards)}, loss = {loss.numpy():.4f}")

    return reward_history
