{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Q-Network (DQN) Demo on LunarLander-v2\n",
        "\n",
        "This notebook demonstrates training and evaluation of a vanilla Deep Q-Network (DQN) agent on the OpenAI Gym **LunarLander-v2** environment.\n",
        "\n",
        "We will cover:\n",
        "- Environment setup and agent architecture\n",
        "- Training the DQN agent\n",
        "- Evaluating agent performance\n",
        "- Plotting results\n",
        "- Key takeaways and conclusion\n"
      ],
      "metadata": {
        "id": "B0xCanneJOdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and setup\n",
        "import gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# Import custom utilities\n",
        "from utils import QNetwork, training_loop, evaluate_agent\n",
        "\n",
        "# Fix seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n"
      ],
      "metadata": {
        "id": "7kINEEDFJQVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment and Hyperparameters\n",
        "\n",
        "We initialize the LunarLander environment and define key hyperparameters for the training.\n"
      ],
      "metadata": {
        "id": "8O6YGTbqJYul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "result = env.reset()\n",
        "if isinstance(result, tuple) and len(result) == 2:\n",
        "    obs, _ = result\n",
        "else:\n",
        "    obs = result\n",
        "\n",
        "input_dim = env.observation_space.shape[0]\n",
        "output_dim = env.action_space.n\n",
        "\n",
        "# Hyperparameters\n",
        "replay_max = 10000\n",
        "learning_rate = 1e-3\n",
        "n_episodes = 500\n",
        "epsilon_min, epsilon_decay = 0.01, 0.995\n",
        "batch_size = 128\n",
        "discounted_factor = 0.99\n",
        "\n",
        "replay_buffer = deque(maxlen=replay_max)\n",
        "\n",
        "# Initialize networks, loss, and optimizer\n",
        "Q_network = QNetwork(input_dim, output_dim)\n",
        "target_network = QNetwork(input_dim, output_dim)\n",
        "target_network.load_state_dict(Q_network.state_dict())\n",
        "target_network.eval()\n",
        "loss_fn = torch.nn.SmoothL1Loss()\n",
        "optimizer = torch.optim.Adam(Q_network.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "y_ht2hXPJfPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Agent\n",
        "\n",
        "We run the training loop for the specified number of episodes. The target network is updated periodically to stabilize learning.\n"
      ],
      "metadata": {
        "id": "ePIHdt3BJkj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = training_loop(env, Q_network, target_network, loss_fn, optimizer, discounted_factor,\n",
        "                        n_episodes, epsilon_decay=epsilon_decay, epsilon_min=epsilon_min)\n"
      ],
      "metadata": {
        "id": "-_-ElbmDJnkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Results\n",
        "\n",
        "The plot below shows the total reward per episode over the training process.\n"
      ],
      "metadata": {
        "id": "bNwL1ZwrJprn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(rewards)\n",
        "plt.title(\"Episode Rewards during Training\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sZsfnODhJsk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Trained Agent\n",
        "\n",
        "Now that training is complete, we evaluate the trained model over multiple episodes without exploration (greedy policy).\n"
      ],
      "metadata": {
        "id": "AYjND84YJyoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_reward = evaluate_agent(env, Q_network, episodes=20, render=False)\n",
        "print(f\"Average Reward over 20 evaluation episodes: {avg_reward:.2f}\")\n"
      ],
      "metadata": {
        "id": "p1XhImNrJz-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "- The vanilla DQN agent successfully learned to solve the LunarLander-v2 task, achieving high average rewards after training.\n",
        "- The use of a replay buffer and target network helps stabilize training.\n",
        "- Further improvements could involve experimenting with Double DQN, prioritized replay, or dueling networks for better performance.\n",
        "- This notebook serves as a foundation for understanding and extending deep reinforcement learning techniques.\n",
        "\n",
        "Feel free to explore the code, modify hyperparameters, or try out the Double DQN variant for enhanced stability and performance!\n"
      ],
      "metadata": {
        "id": "yD7IOh7ZKAfs"
      }
    }
  ]
}