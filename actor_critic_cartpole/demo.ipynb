{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Actor-Critic on CartPole-v1: Training and Demo\n",
        "\n",
        "This notebook implements an **Actor-Critic** reinforcement learning agent to solve the classic CartPole-v1 environment from OpenAI Gym.  \n",
        "We'll train the model, evaluate its performance, and visualize training progress.\n",
        "\n",
        "---\n",
        "\n",
        "## Setup and Imports\n"
      ],
      "metadata": {
        "id": "EwqKJTyuQvnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed = 65\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Create environment\n",
        "env_name = 'CartPole-v1'\n",
        "env = gym.make(env_name)\n",
        "env.seed(seed)\n",
        "env.action_space.seed(seed)\n",
        "\n",
        "print(f\"Environment '{env_name}' created with seed {seed}.\")\n"
      ],
      "metadata": {
        "id": "3sJLCoaTQzRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions\n",
        "\n",
        "We define helper functions for:\n",
        "\n",
        "- Running one episode with the current policy and critic (play_game)  \n",
        "- Evaluating the actor model's performance on the environment (evaluate_episode)  \n",
        "- Calculating discounted rewards and normalizing them for stable training  \n"
      ],
      "metadata": {
        "id": "c2ilAiBwQ3tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_game(env, actor_model, critic_model, gamma=0.99, epsilon=1e-9):\n",
        "    states, actions, rewards, state_values = [], [], [], []\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        states.append(obs)\n",
        "        state_val = critic_model(obs.reshape(1, -1), training=False)[0, 0]\n",
        "        state_values.append(state_val)\n",
        "\n",
        "        p_left = actor_model(obs.reshape(1, -1), training=False)[0, 0]\n",
        "        action = np.random.rand() > p_left\n",
        "        obs, reward, done, _ = env.step(int(action))\n",
        "\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    discounted_rewards = []\n",
        "    G = 0\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        discounted_rewards.insert(0, G)\n",
        "    discounted_rewards = np.array(discounted_rewards)\n",
        "    discounted_rewards_norm = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + epsilon)\n",
        "\n",
        "    return np.array(states), np.array(actions), discounted_rewards_norm, rewards, state_values\n",
        "\n",
        "\n",
        "def evaluate_episode(model, env, max_steps=500):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "\n",
        "    while not done and steps < max_steps:\n",
        "        p_left = model(obs.reshape(1, -1), training=False)[0, 0].numpy()\n",
        "        action = np.random.rand() > p_left\n",
        "        obs, reward, done, _ = env.step(int(action))\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "\n",
        "    return total_reward\n"
      ],
      "metadata": {
        "id": "Z_GqUXZgQ854"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definitions\n",
        "\n",
        "We define:\n",
        "\n",
        "- **Actor model**: outputs probability of moving left (sigmoid output)  \n",
        "- **Critic model**: outputs estimated value of a state (no activation on final layer)  \n"
      ],
      "metadata": {
        "id": "qalViGRIQ_dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_inputs = env.observation_space.shape[0]\n",
        "\n",
        "actor_model = keras.Sequential([\n",
        "    keras.layers.InputLayer(shape=(n_inputs,)),\n",
        "    keras.layers.Dense(5, activation='elu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "critic_model = keras.Sequential([\n",
        "    keras.layers.InputLayer(shape=(n_inputs,)),\n",
        "    keras.layers.Dense(5, activation='elu'),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "print(\"Actor and Critic models defined.\")\n"
      ],
      "metadata": {
        "id": "vlU9-b9LRDoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Setup and Loop\n",
        "\n",
        "We set up:\n",
        "\n",
        "- Loss functions and optimizers for actor and critic  \n",
        "- The main training loop over episodes  \n",
        "- Policy gradient with advantage calculation  \n",
        "- Periodic logging of rewards and losses  \n"
      ],
      "metadata": {
        "id": "HTuF3wTwRKob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function for critic\n",
        "critic_loss_fn = keras.losses.MeanSquaredError()\n",
        "\n",
        "# Optimizers\n",
        "learning_rate = 0.01\n",
        "actor_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "critic_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Training parameters\n",
        "n_episodes = 500\n",
        "gamma = 0.99  # Discount factor\n",
        "epsilon = 1e-9  # Small value to avoid division by zero\n",
        "\n",
        "\n",
        "reward_history = []\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        states, actions, discounted_rewards, rewards, state_values = play_game(env, actor_model, critic_model, gamma, epsilon)\n",
        "\n",
        "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
        "        discounted_rewards = tf.convert_to_tensor(discounted_rewards, dtype=tf.float32)\n",
        "        state_values = tf.convert_to_tensor(state_values, dtype=tf.float32)\n",
        "\n",
        "        # Critic loss: MSE between predicted values and discounted rewards\n",
        "        critic_loss = critic_loss_fn(state_values, discounted_rewards)\n",
        "\n",
        "        # Actor loss: Policy gradient with advantage\n",
        "        p_left = tf.reshape(actor_model(states), (-1,))\n",
        "        action_probs = tf.where(actions == 0, p_left, 1 - p_left)\n",
        "        action_probs = tf.clip_by_value(action_probs, 1e-8, 1.0)\n",
        "\n",
        "        advantage = discounted_rewards - state_values\n",
        "        advantage = (advantage - tf.reduce_mean(advantage)) / (tf.math.reduce_std(advantage) + epsilon)\n",
        "\n",
        "        log_actions = tf.math.log(action_probs + epsilon)\n",
        "        actor_loss = -tf.reduce_mean(log_actions * advantage)\n",
        "\n",
        "    # Compute gradients and apply updates\n",
        "    actor_grads = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "    actor_optimizer.apply_gradients(zip(actor_grads, actor_model.trainable_variables))\n",
        "\n",
        "    critic_grads = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "    critic_optimizer.apply_gradients(zip(critic_grads, critic_model.trainable_variables))\n",
        "\n",
        "    reward_history.append(sum(rewards))\n",
        "\n",
        "    if episode % 50 == 0:\n",
        "        print(f\"Episode {episode:4d} | Reward: {sum(rewards):5.1f} | \"\n",
        "              f\"Actor Loss: {actor_loss.numpy():8.4f} | Critic Loss: {critic_loss.numpy():8.4f}\")\n"
      ],
      "metadata": {
        "id": "uxar49HeRUUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and Visualization\n",
        "\n",
        "After training, we evaluate the trained actor model over multiple episodes to estimate performance.  \n",
        "Finally, we plot the training rewards to visualize learning progress.\n"
      ],
      "metadata": {
        "id": "3wRTOL6ZRmI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate trained actor model\n",
        "test_episodes = 100\n",
        "test_rewards = [evaluate_episode(actor_model, env) for _ in range(test_episodes)]\n",
        "print(f\"\\nAverage reward over {test_episodes} test episodes: {np.mean(test_rewards):.2f}\")\n",
        "\n",
        "# Plot training rewards\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(reward_history, label='Episode Reward')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Training Progress of Actor-Critic on CartPole-v1')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8VjcZShaRaVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "- Our actor-critic agent learns a policy that balances the pole effectively over time.  \n",
        "- The training rewards generally increase, demonstrating improvement in policy quality.  \n",
        "- This simple architecture and training loop provide a solid foundation for policy gradient methods with function approximation.  \n",
        "- Potential improvements include tuning hyperparameters, experimenting with more complex networks, or applying to other environments.\n",
        "\n",
        "---\n",
        "\n",
        "Thanks for following along! Feel free to modify the code and explore further.\n"
      ],
      "metadata": {
        "id": "InBxUZ7dRW_0"
      }
    }
  ]
}