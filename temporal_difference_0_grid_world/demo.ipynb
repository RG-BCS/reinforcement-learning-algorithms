{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Value Learning with TD(0) in a 2D Grid World\n",
        "\n",
        "This notebook demonstrates Temporal-Difference (TD(0)) learning to estimate the value function in a 2D grid world with obstacles.\n",
        "We then use the learned value function to simulate a greedy policy and visualize both the learned values and policy path.\n",
        "\n",
        "## Key Features:\n",
        "- TD(0) Learning\n",
        "- Obstacles in the Grid\n",
        "- Greedy Policy Rollout\n",
        "- Path Visualization\n"
      ],
      "metadata": {
        "id": "R9PAsJ4LT8q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n"
      ],
      "metadata": {
        "id": "ldOU63eAT-ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Grid World and Obstacles\n",
        "\n",
        "We define a 5x6 grid with a goal state and some obstacles. All other states have a step penalty.\n"
      ],
      "metadata": {
        "id": "_AZff3YwUDhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid_world = [5, 6]\n",
        "GOAL_REWARD = 0.0\n",
        "STEP_PENALTY = -1.0\n",
        "\n",
        "OBSTACLES = [(1, 4), (2, 3), (3, 4)]\n",
        "\n",
        "states = []\n",
        "for j in range(grid_world[0]):\n",
        "    for i in range(grid_world[1]):\n",
        "        if (j, i) not in OBSTACLES:\n",
        "            states.append((j, i))\n",
        "\n",
        "state_to_index = {s: idx for idx, s in enumerate(states)}\n",
        "GOAL_STATE = (2, 4)\n",
        "GOAL_INDEX = states.index(GOAL_STATE)\n",
        "\n",
        "actions = np.array(['up', 'down', 'left', 'right'])\n",
        "action_moves = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n"
      ],
      "metadata": {
        "id": "sJLEGtYlUIWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize State Values\n",
        "\n",
        "Set all state values to zero. Obstacle positions are marked as `-inf` to prevent updates.\n"
      ],
      "metadata": {
        "id": "Geb5uJ_7UMff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S_values = np.zeros(grid_world)\n",
        "for (i, j) in OBSTACLES:\n",
        "    S_values[i, j] = -np.inf\n"
      ],
      "metadata": {
        "id": "-dEAi4uFUPfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TD(0) Value Learning\n",
        "\n",
        "We perform value updates using the TD(0) algorithm with a learning rate of 0.1 and discount factor of 1.0.\n"
      ],
      "metadata": {
        "id": "5lIqtzToUSfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_policy_result(current_state, move, g_w=grid_world, OBSTACLES=OBSTACLES):\n",
        "    next_state = current_state[0] + move[0], current_state[1] + move[1]\n",
        "    if next_state not in OBSTACLES and (0 <= next_state[0] < g_w[0]) and (0 <= next_state[1] < g_w[1]):\n",
        "        return next_state, True\n",
        "    else:\n",
        "        return next_state, False\n",
        "\n",
        "learning_rate = 0.1\n",
        "gamma = 1.0\n",
        "n_iteration = 2000\n",
        "\n",
        "for _ in range(n_iteration):\n",
        "    s_val = S_values.copy()\n",
        "    for s in states:\n",
        "        if s == GOAL_STATE:\n",
        "            continue\n",
        "        shuffled_actions = actions.copy()\n",
        "        np.random.shuffle(shuffled_actions)\n",
        "        for action_name in shuffled_actions:\n",
        "            move = action_moves[action_name]\n",
        "            sp, pass_fail = validate_policy_result(s, move)\n",
        "            if pass_fail:\n",
        "                r = GOAL_REWARD if sp == GOAL_STATE else STEP_PENALTY\n",
        "                S_values[s[0], s[1]] = (1 - learning_rate) * s_val[s[0], s[1]] + learning_rate * (r + gamma * s_val[sp[0], sp[1]])\n",
        "                break\n"
      ],
      "metadata": {
        "id": "7Muvl5HIUV44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization and Policy Simulation Helpers\n",
        "Includes:\n",
        "- `print_value_grid`\n",
        "- `plot_arrows_TD0`\n",
        "- `plot_grid_world_TD0`\n",
        "- `simulate_policy_TD0`\n"
      ],
      "metadata": {
        "id": "oDgbjWYZUeyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_value_grid(V):\n",
        "    print(\"\\nValue Grid:\")\n",
        "    for row in range(V.shape[0]):\n",
        "        line = \"\"\n",
        "        for col in range(V.shape[1]):\n",
        "            val = V[row, col]\n",
        "            line += f\"{val:6.2f}  \"\n",
        "        print(line)\n",
        "\n",
        "def plot_arrows_TD0(S_values, grid_world, actions, states, goal_state, OBSTACLES=None, path=None):\n",
        "    action_symbol_map = {'up': '‚Üë', 'down': '‚Üì', 'left': '‚Üê', 'right': '‚Üí'}\n",
        "    action_moves = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
        "\n",
        "    policy_symbols = []\n",
        "    for s in states:\n",
        "        if s == goal_state:\n",
        "            policy_symbols.append('\\U0001F3C6')  # üèÜ Goal\n",
        "            continue\n",
        "        best_action = None\n",
        "        best_value = -np.inf\n",
        "        for action in actions: # check all the actions and find the action that gives max next state value\n",
        "            move = action_moves[action]   # pick an action and get the move increments\n",
        "            sp = s[0] + move[0], s[1] + move[1] # Get the new landing state\n",
        "            if (0 <= sp[0] < grid_world[0]) and (0 <= sp[1] < grid_world[1]) and (sp not in OBSTACLES):\n",
        "                val = S_values[sp[0], sp[1]] # Get the next state value\n",
        "                if val > best_value: # update best value and best action so far for the given state\n",
        "                    best_value = val\n",
        "                    best_action = action\n",
        "        symbol = action_symbol_map[best_action] if best_action else ' ' # Pick the arrow direction\n",
        "        policy_symbols.append(symbol)\n",
        "\n",
        "    # Build visual grid\n",
        "    symbol_grid = []\n",
        "    OBSTACLE_SYMBOL = '\\u2588'\n",
        "    START_STATE_SYMBOL = \"\\u26F3\"\n",
        "    idx = 0\n",
        "    for i in range(grid_world[0]):\n",
        "        row = []\n",
        "        for j in range(grid_world[1]):\n",
        "            if OBSTACLES and (i, j) in OBSTACLES:\n",
        "                row.append(OBSTACLE_SYMBOL)\n",
        "            else:\n",
        "                row.append(policy_symbols[idx])\n",
        "                idx += 1\n",
        "        symbol_grid.append(row)\n",
        "\n",
        "    # Overlay path if provided. This is when user specifies a start state\n",
        "    STEP_MARK = '\\u25CF'\n",
        "    if path:\n",
        "        for step_num, (i, j) in enumerate(path):\n",
        "            if (i, j) == path[-1]:  # goal state already marked so do nothing\n",
        "                continue\n",
        "            if (i, j) == path[0]:   # start cell/state\n",
        "                symbol_grid[i][j] = START_STATE_SYMBOL\n",
        "            else:\n",
        "                symbol_grid[i][j] = str(step_num % 10)\n",
        "\n",
        "    print(\"Policy (greedy from state-values):\")\n",
        "    for row in symbol_grid:\n",
        "        print(''.join(f'{cell:^6}' for cell in row))\n",
        "    print()\n",
        "\n",
        "def plot_grid_world_TD0(states, grid_size, obstacles, goal, path=None, S_values=None, actions=None):\n",
        "    GOAL_STATE_SYMBOL = '\\U0001F3C6'     # üèÜ Goal\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlim(0, grid_size[1])\n",
        "    ax.set_ylim(0, grid_size[0])\n",
        "    ax.set_xticks(range(grid_size[1]+1))\n",
        "    ax.set_yticks(range(grid_size[0]+1))\n",
        "    ax.set_aspect('equal')\n",
        "    ax.invert_yaxis()  # So (0,0) is top-left\n",
        "\n",
        "    # Draw grid\n",
        "    for i in range(grid_size[0]):\n",
        "        for j in range(grid_size[1]):\n",
        "            rect = patches.Rectangle((j, i), 1, 1, linewidth=1, edgecolor='gray', facecolor='white')\n",
        "            ax.add_patch(rect)\n",
        "            if (i, j) in obstacles:\n",
        "                rect.set_facecolor('black')\n",
        "            elif (i, j) == goal:\n",
        "                ax.text(j+0.5, i+0.5, \"G:\\u2605\", ha='center', va='center', fontsize=20, color='red')\n",
        "\n",
        "    # Draw policy arrows if Q-values are provided\n",
        "    if S_values is not None and actions is not None:\n",
        "        action_to_vector = {\n",
        "            'up': (0, -0.3),\n",
        "            'down': (0, 0.3),\n",
        "            'left': (-0.3, 0),\n",
        "            'right': (0.3, 0)\n",
        "        }\n",
        "        for idx, state in enumerate(states):\n",
        "            if state == goal:\n",
        "                continue\n",
        "            best_action = None\n",
        "            best_value = -np.inf\n",
        "            for action_name in actions:\n",
        "                move = action_moves[action_name]\n",
        "                sp = state[0]+move[0],state[1]+move[1]\n",
        "                if (0<= sp[0] < grid_size[0]) and (0<= sp[1]<grid_size[1]) and (sp not in obstacles):\n",
        "                    val = S_values[sp[0],sp[1]]\n",
        "                    if val > best_value:\n",
        "                        best_value = val\n",
        "                        best_action = action_name\n",
        "            #best_action = actions[np.argmax(Q_values[idx])]\n",
        "            dx, dy = action_to_vector[best_action]\n",
        "            x, y = state[1] + 0.5, state[0] + 0.5\n",
        "            ax.arrow(x, y, dx, dy, head_width=0.1, head_length=0.1, fc='blue', ec='blue')\n",
        "\n",
        "    # Draw path\n",
        "    if path:\n",
        "        for idx, (i, j) in enumerate(path):\n",
        "            if (i, j) != goal:\n",
        "                ax.text(j+0.5, i+0.5, str(idx%10), ha='center', va='center', fontsize=20, color='green')\n",
        "\n",
        "    ax.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# this is called when the user specifies a start position\n",
        "def simulate_policy_TD0(S_values, states, actions, start_index, goal_index, action_moves, OBSTACLES, grid_world):\n",
        "    path = [states[start_index]]\n",
        "    current_state = states[start_index]\n",
        "    current_index = start_index\n",
        "    g_w = grid_world\n",
        "    while current_index != goal_index:\n",
        "        best_value = -np.inf\n",
        "        best_next_state = None\n",
        "        for action in actions:\n",
        "            move = action_moves[action]\n",
        "            next_state = (current_state[0] + move[0], current_state[1] + move[1])\n",
        "            # Check if next_state valid\n",
        "            if next_state not in OBSTACLES and 0 <= next_state[0] < g_w[0] and 0 <= next_state[1] < g_w[1]:\n",
        "                next_value = S_values[next_state[0], next_state[1]] # Get the next state value\n",
        "                if next_value > best_value:  # update the next state choice based on max state value\n",
        "                    best_value = next_value\n",
        "                    best_next_state = next_state\n",
        "\n",
        "        if best_next_state is None:\n",
        "            print(\"No valid moves from current state. Stuck!\")\n",
        "            break\n",
        "\n",
        "        current_state = best_next_state\n",
        "        current_index = state_to_index[current_state]\n",
        "        path.append(current_state)\n",
        "\n",
        "        # Optional: Break if stuck in a loop (avoid infinite loops)\n",
        "        if len(path) > 100:\n",
        "            print(\"Too long path, stopping to avoid infinite loop\")\n",
        "            break\n",
        "\n",
        "    return path"
      ],
      "metadata": {
        "id": "3sbu438pUgNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Greedy Policy from State Values\n"
      ],
      "metadata": {
        "id": "10ksPK_ZVA_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_arrows_TD0(S_values, grid_world, actions, states, goal_state=GOAL_STATE, path=None, OBSTACLES=OBSTACLES)"
      ],
      "metadata": {
        "id": "pmcgUKzxVEAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulate and Visualize Path from Start to Goal\n"
      ],
      "metadata": {
        "id": "izzAADY5VJLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "START_STATE = (2, 0)\n",
        "START_INDEX = states.index(START_STATE)\n",
        "\n",
        "path = simulate_policy_TD0(\n",
        "    S_values, states, actions,\n",
        "    start_index=START_INDEX,\n",
        "    goal_index=GOAL_INDEX,\n",
        "    action_moves=action_moves,\n",
        "    OBSTACLES=OBSTACLES,\n",
        "    grid_world=grid_world\n",
        ")\n",
        "\n",
        "plot_arrows_TD0(S_values, grid_world, actions, states, goal_state=GOAL_STATE, path=path, OBSTACLES=OBSTACLES)\n"
      ],
      "metadata": {
        "id": "YB5_WtB3VKvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Grid Visualization with Arrows and Path\n"
      ],
      "metadata": {
        "id": "4HyTDFV4VRH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_grid_world_TD0(\n",
        "    states, grid_size=grid_world,\n",
        "    obstacles=OBSTACLES, goal=GOAL_STATE,\n",
        "    path=path, S_values=S_values, actions=actions\n",
        ")\n"
      ],
      "metadata": {
        "id": "ietfNfvYVStn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary & Conclusion\n",
        "\n",
        "In this notebook, we implemented **Temporal-Difference (TD(0)) value learning** in a 2D grid world environment featuring obstacles and a goal state.\n",
        "\n",
        "### What we did:\n",
        "- Defined a grid world with obstacles and rewards.\n",
        "- Applied the **TD(0) learning algorithm** to estimate state values over time.\n",
        "- Derived a **greedy policy** based on learned state values.\n",
        "- Simulated and visualized the **agent‚Äôs path** from a start state to the goal.\n",
        "- Plotted the **value grid and policy arrows** to gain insight into the agent‚Äôs decision-making.\n",
        "\n",
        "### Key Takeaways:\n",
        "- TD(0) learning does not require a model of the environment.\n",
        "- Random action shuffling enables exploration during value updates.\n",
        "- Even without explicitly learning a policy, a **greedy policy** can emerge from state values.\n",
        "- Visualization is a powerful tool for interpreting the results of RL algorithms.\n",
        "\n",
        "This basic implementation sets the stage for more advanced techniques such as **Q-learning**, **SARSA**, or **Monte Carlo methods**, which consider action values or trajectories.\n",
        "\n",
        "Happy Reinforcement Learning!\n"
      ],
      "metadata": {
        "id": "E7MMWumNVVUC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RE1LR7gAVnHH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}