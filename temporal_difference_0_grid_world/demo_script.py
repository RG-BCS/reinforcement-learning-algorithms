# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

"""
Demo script to visualize TD(0) value learning and greedy policy in a 2D grid world.
"""

from temporal_difference_0_grid_world import (
    S_values,
    grid_world,
    actions,
    states,
    GOAL_STATE,
    GOAL_INDEX,
    OBSTACLES,
    state_to_index,
    simulate_policy_TD0,
    plot_arrows_TD0,
    plot_grid_world_TD0,
)

def run_demo(start_state=(2, 0)):
    """
    Runs the policy rollout and visualization from a given start state.
    """
    print(f"\nStarting TD(0) policy simulation from start state: {start_state}")

    start_index = state_to_index[start_state]
    path = simulate_policy_TD0(
        S_values,
        states,
        actions,
        start_index=start_index,
        goal_index=GOAL_INDEX,
        action_moves={
            'up': (-1, 0),
            'down': (1, 0),
            'left': (0, -1),
            'right': (0, 1)
        },
        OBSTACLES=OBSTACLES,
        grid_world=grid_world
    )

    # Print policy with arrows and annotated path
    plot_arrows_TD0(
        S_values, grid_world, actions, states,
        goal_state=GOAL_STATE, path=path, OBSTACLES=OBSTACLES
    )

    # Visual grid world with path and learned state values
    plot_grid_world_TD0(
        states, grid_size=grid_world, obstacles=OBSTACLES,
        goal=GOAL_STATE, path=path, S_values=S_values, actions=actions
    )


if __name__ == "__main__":
    run_demo(start_state=(2, 0))