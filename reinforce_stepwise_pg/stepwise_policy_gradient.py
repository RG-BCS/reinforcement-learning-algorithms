# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

"""
stepwise_policy_gradient.py

Implementation of a Stepwise Policy Gradient Reinforcement Learning algorithm
applied to the CartPole-v1 environment.

The policy is represented as a simple neural network trained using the REINFORCE
algorithm with discounted rewards.

Author: Your Name
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
import gym


class StepwisePolicyGradientAgent:
    """
    Reinforcement learning agent using stepwise policy gradient (REINFORCE).

    Attributes:
        env_name (str): Gym environment name.
        model (keras.Model): Policy network mapping observations to action probabilities.
        loss_fn: Loss function (binary cross-entropy).
        optimizer: Optimizer instance.
        discount_factor (float): Discount factor for rewards.
        seed (int): Random seed for reproducibility.
    """

    def __init__(self, env_name, model, loss_fn, optimizer, discount_factor=0.95, seed=42):
        self.env_name = env_name
        self.model = model
        self.loss_fn = loss_fn
        self.optimizer = optimizer
        self.discount_factor = discount_factor
        self.seed = seed

        # Set seeds for reproducibility
        self._set_seeds(seed)

    def _set_seeds(self, seed):
        np.random.seed(seed)
        tf.random.set_seed(seed)
        self.env = gym.make(self.env_name)
        self.env.seed(seed)
        self.env.action_space.seed(seed)

    def basic_policy(self, obs):
        """
        A hardcoded baseline policy:
        Accelerate left (0) if pole angle < 0, else accelerate right (1).

        Args:
            obs (np.array): Observation from environment (state).

        Returns:
            int: Action chosen by the policy.
        """
        angle = obs[2]
        return 0 if angle < 0 else 1

    def evaluate_policy(self, n_eval_episodes=5):
        """
        Evaluate the current policy over multiple episodes.

        Args:
            n_eval_episodes (int): Number of episodes to evaluate.

        Returns:
            float: Average total reward over evaluation episodes.
            list: List of total rewards per episode.
        """
        total_rewards = []
        for _ in range(n_eval_episodes):
            obs = self.env.reset()
            done = False
            total_reward = 0
            while not done:
                obs_tensor = tf.convert_to_tensor(obs[np.newaxis].astype(np.float32))
                proba = self.model(obs_tensor)
                action = int(proba.numpy()[0, 0] < 0.5)  # deterministic action
                obs, reward, done, _ = self.env.step(action)
                total_reward += reward
            total_rewards.append(total_reward)
        return np.mean(total_rewards), total_rewards

    def play_one_step(self, obs):
        """
        Play one step in the environment using the current policy and calculate gradients.

        Args:
            obs (np.array): Current observation.

        Returns:
            obs (np.array): Next observation.
            reward (float): Reward received.
            done (bool): Whether the episode finished.
            grads (list): Gradients from this step.
        """
        with tf.GradientTape() as tape:
            obs_tensor = tf.reshape(tf.convert_to_tensor(obs, dtype=tf.float32), (1, -1))
            left_proba = self.model(obs_tensor)
            action = tf.random.uniform([1, 1]) > left_proba
            y_target = tf.constant([[1.]]) - tf.cast(action, dtype=tf.float32)
            loss = tf.reduce_mean(self.loss_fn(y_target, left_proba))

        grads = tape.gradient(loss, self.model.trainable_variables)
        obs, reward, done, _ = self.env.step(int(action[0, 0]))
        return obs, reward, done, grads

    @staticmethod
    def discount_and_normalize_rewards(all_rewards, discount_factor, epsilon=1e-9):
        """
        Compute discounted rewards and normalize them across episodes.

        Args:
            all_rewards (list of lists): Rewards per episode.
            discount_factor (float): Discount factor gamma.
            epsilon (float): Small value to avoid division by zero.

        Returns:
            list of np.array: Normalized discounted rewards per episode.
        """
        all_norm_rewards = []
        for rewards in all_rewards:
            G = 0
            discounted = []
            for r in rewards[::-1]:
                G = r + discount_factor * G
                discounted.insert(0, G)
            all_norm_rewards.append(np.array(discounted))

        flat_rewards = np.concatenate(all_norm_rewards)
        reward_mean = flat_rewards.mean()
        reward_std = flat_rewards.std()

        return [(disc_rewards - reward_mean) / (reward_std + epsilon) for disc_rewards in all_norm_rewards]

    def play_multiple_episodes(self, n_episodes, n_max_steps):
        """
        Play multiple episodes to gather rewards and gradients.

        Args:
            n_episodes (int): Number of episodes to play.
            n_max_steps (int): Maximum steps per episode.

        Returns:
            all_rewards (list): Rewards collected.
            all_grads (list): Gradients collected.
        """
        all_rewards = []
        all_grads = []
        for _ in range(n_episodes):
            current_rewards = []
            current_grads = []
            obs = self.env.reset()
            for _ in range(n_max_steps):
                obs, reward, done, grads = self.play_one_step(obs)
                current_rewards.append(reward)
                current_grads.append(grads)
                if done:
                    break
            all_rewards.append(current_rewards)
            all_grads.append(current_grads)
        return all_rewards, all_grads

    def train(self, n_iterations, n_episodes_per_update, n_max_steps):
        """
        Train the policy network using REINFORCE.

        Args:
            n_iterations (int): Number of training iterations.
            n_episodes_per_update (int): Episodes per gradient update.
            n_max_steps (int): Max steps per episode.

        Returns:
            list: Average evaluation rewards per iteration.
        """
        eval_rewards = []

        for iteration in range(n_iterations):
            all_rewards, all_grads = self.play_multiple_episodes(n_episodes_per_update, n_max_steps)
            all_final_rewards = self.discount_and_normalize_rewards(all_rewards, self.discount_factor)

            mean_grads = []
            for var_index in range(len(self.model.trainable_variables)):
                grads_for_var = []
                for ep_idx, final_rewards in enumerate(all_final_rewards):
                    for step, reward in enumerate(final_rewards):
                        grads_for_var.append(reward * all_grads[ep_idx][step][var_index])
                mean_grad = tf.reduce_mean(grads_for_var, axis=0)
                mean_grads.append(mean_grad)

            self.optimizer.apply_gradients(zip(mean_grads, self.model.trainable_variables))

            if iteration % 10 == 0 or iteration == n_iterations - 1:
                avg_reward, _ = self.evaluate_policy(n_eval_episodes=10)
                eval_rewards.append(avg_reward)
                print(f"Iteration {iteration}: Average test reward over 10 episodes: {avg_reward:.2f}")

        return eval_rewards


def create_policy_network(n_inputs=4,n_hidden=5):
    """
    Create the policy network model.

    Args:
        n_inputs (int): Number of input features.

    Returns:
        keras.Model: Compiled policy network.
    """
    model = keras.Sequential([
        keras.layers.InputLayer(shape=(n_inputs,)),
        keras.layers.Dense(n_hidden, activation='elu'),
        keras.layers.Dense(1, activation='sigmoid')
    ])
    return model


if __name__ == "__main__":
    # For quick tests or debugging here if desired
    pass
