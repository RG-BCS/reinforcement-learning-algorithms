{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stepwise Policy Gradient Reinforcement Learning on CartPole-v1\n",
        "\n",
        "This notebook demonstrates training a reinforcement learning agent using the **Stepwise Policy Gradient (REINFORCE)** algorithm on the classic OpenAI Gym environment **CartPole-v1**.\n",
        "\n",
        "---\n",
        "\n",
        "## Objective\n",
        "\n",
        "The goal is to keep the pole balanced upright on the cart as long as possible. The maximum reward per episode is 500 steps.\n",
        "\n",
        "We will:\n",
        "\n",
        "- Evaluate a **hardcoded baseline policy** that chooses actions based on pole angle.\n",
        "- Train a **neural network policy** using policy gradients.\n",
        "- Compare performance before and after training.\n",
        "- Visualize training progress.\n"
      ],
      "metadata": {
        "id": "OJOkZpPiPim2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and setup\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "from stepwise_policy_gradient import StepwisePolicyGradientAgent, create_policy_network"
      ],
      "metadata": {
        "id": "ZiQOejuAPjyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Baseline Hardcoded Policy\n",
        "\n",
        "We start with a simple policy that accelerates left if the pole is leaning left (angle < 0), else right.\n",
        "\n",
        "This will serve as a baseline for comparison.\n"
      ],
      "metadata": {
        "id": "zQYGenmMQF95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_policy(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "rewards = []\n",
        "n_episodes = 500\n",
        "for episode in range(n_episodes):\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = basic_policy(obs)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "rewards = np.array(rewards)\n",
        "print(f\"Hardcoded policy average reward over {n_episodes} episodes: {rewards.mean():.2f}\")\n",
        "print(f\"Min reward: {rewards.min()}, Max reward: {rewards.max()}\")"
      ],
      "metadata": {
        "id": "4f3aLWrhQIZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Initialize and Evaluate Untrained Neural Network Policy\n",
        "\n",
        "Now, we create a neural network policy and evaluate its performance **before training**.\n"
      ],
      "metadata": {
        "id": "MfkkE4m1QN43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "policy_net = create_policy_network(n_inputs=4)\n",
        "\n",
        "agent = StepwisePolicyGradientAgent(\n",
        "    env_name='CartPole-v1',\n",
        "    model=policy_net,\n",
        "    loss_fn=tf.keras.losses.binary_crossentropy,\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    discount_factor=0.95,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "mean_reward_before, _ = agent.evaluate_policy(n_eval_episodes=50)\n",
        "print(f\"Untrained policy average reward over 50 episodes: {mean_reward_before:.2f}\")\n"
      ],
      "metadata": {
        "id": "jdNDG0sNQQvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train the Neural Network Policy using Policy Gradient\n",
        "\n",
        "We train for 150 iterations, updating the policy after every 10 episodes.\n",
        "\n",
        "Training progress will be printed and saved for visualization.\n"
      ],
      "metadata": {
        "id": "6mu-6sWNQXhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_iterations = 150\n",
        "n_episodes_per_update = 10\n",
        "n_max_steps = gym.make('CartPole-v1').spec.max_episode_steps\n",
        "\n",
        "eval_rewards = agent.train(\n",
        "    n_iterations=n_iterations,\n",
        "    n_episodes_per_update=n_episodes_per_update,\n",
        "    n_max_steps=n_max_steps\n",
        ")\n"
      ],
      "metadata": {
        "id": "4A_pysh8QakZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Evaluate the Trained Policy\n",
        "\n",
        "We evaluate the trained policy over 50 episodes and compare it to the baseline.\n"
      ],
      "metadata": {
        "id": "GNcCZiJ3QlJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_reward_after, rewards_after = agent.evaluate_policy(n_eval_episodes=50)\n",
        "print(f\"Trained policy average reward over 50 episodes: {mean_reward_after:.2f}\")"
      ],
      "metadata": {
        "id": "k1o3qeX0Qmjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Visualize Training Progress\n",
        "\n",
        "Plot average reward during training to see how the policy improves over time.\n"
      ],
      "metadata": {
        "id": "TfR4hpFcQqwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = list(range(0, n_iterations, 10)) + [n_iterations - 1]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(iterations, eval_rewards, label='Policy Training Reward')\n",
        "plt.xlabel('Training Iteration')\n",
        "plt.ylabel('Average Reward over 10 Episodes')\n",
        "plt.title('Policy Performance Over Training Iterations')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o6_6tQ96QtnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Summary\n",
        "\n",
        "- The **hardcoded policy** provides a baseline reward score.\n",
        "- The **untrained neural network policy** starts near random performance.\n",
        "- Training using the **stepwise policy gradient** significantly improves the policy.\n",
        "- Visualizing rewards confirms successful learning toward the maximum reward of 500.\n",
        "\n",
        "---\n",
        "\n",
        "This concludes the demonstration of the Stepwise Policy Gradient method for reinforcement learning on CartPole-v1.\n"
      ],
      "metadata": {
        "id": "qESjDjvfQwtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XbXIY0rcQ0ij"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}