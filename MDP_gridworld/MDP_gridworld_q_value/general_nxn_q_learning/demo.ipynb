{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning on an NxN GridWorld (No Obstacles)\n",
        "\n",
        "This notebook demonstrates Q-Learning applied to a deterministic NxN grid world environment.\n",
        "\n",
        "## Problem Setup\n",
        "- The agent starts in any state in a grid.\n",
        "- The goal is to reach the bottom-right corner of the grid (goal state).\n",
        "- Rewards:\n",
        "  - +10 for reaching the goal state.\n",
        "  - -1 penalty for every other move to encourage shortest path.\n",
        "- Actions: Up, Down, Left, Right.\n",
        "- Transition is deterministic — actions succeed unless hitting grid boundary (agent stays in place).\n",
        "- Discount factor γ = 0.98.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Import Required Libraries and Setup\n"
      ],
      "metadata": {
        "id": "aGpX5VEPdo_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from q_learning_nxn import QLearningNxN  # Import the class from the main code\n"
      ],
      "metadata": {
        "id": "tcQjCdBIdq4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Initialize Q-Learning Environment and Parameters\n"
      ],
      "metadata": {
        "id": "Ua094aLIdyfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid size\n",
        "grid_size = (5, 5)\n",
        "\n",
        "# Create QLearning instance\n",
        "q_learning_agent = QLearningNxN(grid_size=grid_size, gamma=0.98)\n",
        "\n",
        "print(f\"Grid size: {grid_size[0]} rows x {grid_size[1]} columns\")\n",
        "print(f\"Total states: {len(q_learning_agent.states)}\")\n",
        "print(f\"Goal state: {q_learning_agent.goal_state}\")\n"
      ],
      "metadata": {
        "id": "AKqYufSMd2Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training the Q-Learning Agent\n",
        "\n",
        "We will train for 50 iterations and observe how the policy improves.\n"
      ],
      "metadata": {
        "id": "vO9PSfvVd-u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the agent\n",
        "n_iterations = 50\n",
        "q_learning_agent.train(n_iterations=n_iterations)\n",
        "\n",
        "print(f\"Training completed with {n_iterations} iterations.\")\n"
      ],
      "metadata": {
        "id": "1jNL77f0d-aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Display the Learned Policy\n",
        "\n",
        "Here, arrows indicate the optimal action for each state, and `G` indicates the goal state.\n"
      ],
      "metadata": {
        "id": "ZOjKebPdeH_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_learning_agent.print_policy()\n"
      ],
      "metadata": {
        "id": "BIYzqlHHeKgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Visualize Q-Values for Each Action\n",
        "\n",
        "We plot heatmaps of the Q-values for each action to understand the agent's value estimates.\n"
      ],
      "metadata": {
        "id": "x_uk20A1ePCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actions = q_learning_agent.actions\n",
        "Q_values = q_learning_agent.Q_values\n",
        "grid_shape = grid_size\n",
        "\n",
        "fig, axs = plt.subplots(1, len(actions), figsize=(20, 4))\n",
        "for i, action in enumerate(actions):\n",
        "    ax = axs[i]\n",
        "    q_vals_action = Q_values[:, i].reshape(grid_shape)\n",
        "    im = ax.imshow(q_vals_action, cmap='coolwarm', interpolation='nearest')\n",
        "    ax.set_title(f\"Q-values for '{action}'\")\n",
        "    ax.set_xticks(np.arange(grid_shape[1]))\n",
        "    ax.set_yticks(np.arange(grid_shape[0]))\n",
        "    for (j, k), val in np.ndenumerate(q_vals_action):\n",
        "        ax.text(k, j, f\"{val:.1f}\", ha='center', va='center', color='black')\n",
        "    fig.colorbar(im, ax=ax)\n",
        "\n",
        "plt.suptitle(\"Q-Values Heatmaps for Each Action\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oBBHD1SyeSXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "- The agent learns an optimal policy that guides it to the goal efficiently.\n",
        "- The Q-values give insight into the expected return of taking each action in each state.\n",
        "- This simple deterministic environment helps understand basic Q-Learning mechanics.\n",
        "\n",
        "---\n",
        "\n",
        "Next steps: Extend to include obstacles and stochastic transitions.\n"
      ],
      "metadata": {
        "id": "iXtzmXKCeY0R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BZuXd4fteYaz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}