{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning in a Flexible NxN GridWorld with Obstacles\n",
        "\n",
        "This notebook demonstrates how to compute an optimal policy using Q-values (value iteration) in a deterministic GridWorld environment. It supports:\n",
        "- Arbitrary grid sizes (`NxN`)\n",
        "- Configurable obstacles\n",
        "- Any custom goal position\n",
        "- Visualization of the learned policy and path from any start point\n"
      ],
      "metadata": {
        "id": "AS0lKBRHlV-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Grid settings\n",
        "grid_world = [6, 6]  # rows, cols\n",
        "GOAL_REWARD = 10.0\n",
        "STEP_PENALTY = -1.0\n",
        "OBSTACLES = [(1, 1), (2, 1), (3, 1), (4, 3), (4, 4), (1, 4), (2, 4), (3, 4)]\n",
        "GOAL_STATE = (0, 5)\n"
      ],
      "metadata": {
        "id": "savELzTHlzUz"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Construction\n",
        "Create a list of valid states (excluding obstacles), and set up the transition and reward matrices.\n"
      ],
      "metadata": {
        "id": "YqAffncwl6M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actions = np.array(['up', 'down', 'left', 'right'])\n",
        "action_moves = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
        "\n",
        "def build_environment(grid_size, obstacles, goal_state):\n",
        "    states = [(j, i) for j in range(grid_size[0]) for i in range(grid_size[1]) if (j, i) not in obstacles]\n",
        "    assert goal_state in states, \"Goal state must be valid and not an obstacle.\"\n",
        "    goal_index = states.index(goal_state)\n",
        "    return states, goal_index\n",
        "\n",
        "states, GOAL_INDEX = build_environment(grid_world, OBSTACLES, GOAL_STATE)"
      ],
      "metadata": {
        "id": "o7wRYEfil-ew"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Value Initialization and Update (Value Iteration)\n",
        "This uses value iteration to compute the Q-values based on a deterministic transition model.\n"
      ],
      "metadata": {
        "id": "3M9rROOgmDse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values = np.zeros((len(states), len(actions)))\n",
        "transition_probability = np.zeros((len(states), len(actions), len(states)))\n",
        "reward_matrix = np.zeros((len(states), len(actions), len(states)))\n",
        "\n",
        "for i, s in enumerate(states):\n",
        "    if i == GOAL_INDEX:\n",
        "        continue\n",
        "    for j, a in enumerate(actions):\n",
        "        move = action_moves[a]\n",
        "        next_state = (s[0] + move[0], s[1] + move[1])\n",
        "        next_state_val = (max(0, min(next_state[0], grid_world[0] - 1)),\n",
        "                          max(0, min(next_state[1], grid_world[1] - 1)))\n",
        "        if next_state_val in OBSTACLES:\n",
        "            next_state_val = s\n",
        "        sp_index = states.index(next_state_val)\n",
        "        transition_probability[i, j, sp_index] = 1.0\n",
        "        reward_matrix[i, j, sp_index] = GOAL_REWARD if sp_index == GOAL_INDEX else STEP_PENALTY\n",
        "\n",
        "gamma = 0.90\n",
        "for _ in range(50):\n",
        "    q_old = Q_values.copy()\n",
        "    for s in range(len(states)):\n",
        "        for a in range(len(actions)):\n",
        "            Q_values[s, a] = sum([\n",
        "                transition_probability[s, a, sp] *\n",
        "                (reward_matrix[s, a, sp] + gamma * np.max(q_old[sp]))\n",
        "                for sp in range(len(states))\n",
        "            ])\n"
      ],
      "metadata": {
        "id": "Ryr_3EGrmGaR"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the Optimal Policy\n",
        "Arrows indicate the optimal action to take from each cell, except obstacles and the goal.\n"
      ],
      "metadata": {
        "id": "M5wAhpO-mLyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_arrows(Q_values, grid_world, actions, states, goal_state, path=None, OBSTACLES=None):\n",
        "    action_symbol_map = {'up': '‚Üë','down': '‚Üì','left': '‚Üê','right': '‚Üí'}\n",
        "    action_symbols = [action_symbol_map[action] for action in actions]\n",
        "\n",
        "    policy_indices = np.argmax(Q_values, axis=1)\n",
        "    policy_symbols = [action_symbols[i] for i in policy_indices]\n",
        "\n",
        "    # FIX: Replace the goal state symbol instead of inserting\n",
        "    goal_idx = states.index(goal_state)\n",
        "    policy_symbols[goal_idx] = '\\U0001F3C6'  # üèÜ Trophy symbol\n",
        "\n",
        "    symbol_grid = []\n",
        "    OBSTACLE_SYMBOL = '\\u2588'\n",
        "    START_STATE_SYMBOL = \"\\u26F3\"\n",
        "    idx = 0\n",
        "\n",
        "    for i in range(grid_world[0]):\n",
        "        row = []\n",
        "        for j in range(grid_world[1]):\n",
        "            if OBSTACLES and (i, j) in OBSTACLES:\n",
        "                row.append(OBSTACLE_SYMBOL)\n",
        "            elif (i, j) in states:\n",
        "                row.append(policy_symbols[idx])\n",
        "                idx += 1\n",
        "            else:\n",
        "                row.append(' ')  # Empty if not in states (e.g., removed due to obstacle)\n",
        "        symbol_grid.append(row)\n",
        "\n",
        "    # Overlay path if provided\n",
        "    if path:\n",
        "        for step_num, (i, j) in enumerate(path):\n",
        "            if (i, j) == goal_state:\n",
        "                continue\n",
        "            elif (i, j) == path[0]:\n",
        "                symbol_grid[i][j] = START_STATE_SYMBOL  # Start flag\n",
        "            else:\n",
        "                symbol_grid[i][j] = str(step_num % 10)  # Step number\n",
        "\n",
        "    print(\"Optimal Policy Grid:\")\n",
        "    for row in symbol_grid:\n",
        "        print('    '.join(row))\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "def plot_grid_world(states, grid_size, obstacles, goal, path=None, Q_values=None, actions=None):\n",
        "    GOAL_SYMBOL = '\\U0001F3C6'\n",
        "    fig, ax = plt.subplots(figsize=(grid_size[1], grid_size[0]))\n",
        "    ax.set_xlim(0, grid_size[1])\n",
        "    ax.set_ylim(0, grid_size[0])\n",
        "    ax.set_xticks(range(grid_size[1] + 1))\n",
        "    ax.set_yticks(range(grid_size[0] + 1))\n",
        "    ax.set_aspect('equal')\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "    # Draw grid cells\n",
        "    for i in range(grid_size[0]):\n",
        "        for j in range(grid_size[1]):\n",
        "            rect = patches.Rectangle((j, i), 1, 1, linewidth=1, edgecolor='gray', facecolor='white')\n",
        "            ax.add_patch(rect)\n",
        "            if (i, j) in obstacles:\n",
        "                rect.set_facecolor('black')\n",
        "            elif (i, j) == goal:\n",
        "                ax.text(j+0.5, i+0.5, \"G:\\u2605\", ha='center', va='center', fontsize=20, color='red')\n",
        "\n",
        "    # Draw policy arrows\n",
        "    if Q_values is not None and actions is not None:\n",
        "        action_to_vector = {\n",
        "            'up': (0, -0.3),\n",
        "            'down': (0, 0.3),\n",
        "            'left': (-0.3, 0),\n",
        "            'right': (0.3, 0)\n",
        "        }\n",
        "        for idx, state in enumerate(states):\n",
        "            if state == goal:\n",
        "                continue\n",
        "            best_action = actions[np.argmax(Q_values[idx])]\n",
        "            dx, dy = action_to_vector[best_action]\n",
        "            x, y = state[1] + 0.5, state[0] + 0.5\n",
        "            ax.arrow(x, y, dx, dy, head_width=0.1, head_length=0.1, fc='blue', ec='blue')\n",
        "\n",
        "    # Draw path\n",
        "    if path:\n",
        "        for idx, (i, j) in enumerate(path):\n",
        "            if (i, j) != goal:\n",
        "                ax.text(j + 0.5, i + 0.5, str(idx % 10), ha='center', va='center', fontsize=14, color='green')\n",
        "\n",
        "    ax.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_arrows(Q_values, grid_world, actions, states, goal_state=GOAL_STATE, path=None, OBSTACLES=OBSTACLES)\n",
        "plot_grid_world(states, grid_world, OBSTACLES, GOAL_STATE, path=None, Q_values=Q_values, actions=actions)"
      ],
      "metadata": {
        "id": "a_KDR5sLmQRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulate a Policy Rollout\n",
        "Given a start location, roll out the deterministic policy until the goal is reached.\n"
      ],
      "metadata": {
        "id": "_5BsarptmUmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_policy(Q_values, states, actions, start_index, goal_index):\n",
        "    path = [states[start_index]]\n",
        "    current_index = start_index\n",
        "    while current_index != goal_index:\n",
        "        best_action = np.argmax(Q_values[current_index])\n",
        "        move = action_moves[actions[best_action]]\n",
        "        next_state = (states[current_index][0] + move[0], states[current_index][1] + move[1])\n",
        "        next_state = (max(0, min(next_state[0], grid_world[0] - 1)),\n",
        "                      max(0, min(next_state[1], grid_world[1] - 1)))\n",
        "        if next_state in OBSTACLES:\n",
        "            next_state = states[current_index]\n",
        "        current_index = states.index(next_state)\n",
        "        path.append(states[current_index])\n",
        "    return path\n",
        "\n",
        "START_STATE = (2, 2)\n",
        "START_INDEX = states.index(START_STATE)\n",
        "path = simulate_policy(Q_values, states, actions, START_INDEX, GOAL_INDEX)\n",
        "plot_arrows(Q_values, grid_world, actions, states, goal_state=GOAL_STATE, path=path, OBSTACLES=OBSTACLES)\n",
        "plot_grid_world(states, grid_world, OBSTACLES, GOAL_STATE, path=path, Q_values=Q_values, actions=actions)"
      ],
      "metadata": {
        "id": "Sjtp8soLmWqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QVtzniQHpiOC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}