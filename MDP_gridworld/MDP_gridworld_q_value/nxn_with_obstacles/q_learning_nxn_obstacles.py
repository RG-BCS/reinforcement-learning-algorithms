# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

"""
Q-Learning for NxN GridWorld with Optional Obstacles
Author: [Your Name]
Description:
    This module implements a deterministic model-based Q-Learning algorithm for solving
    GridWorld environments with or without obstacles.
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# Global Parameters
GOAL_REWARD = 10.0
STEP_PENALTY = -1.0
action_moves = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}
action_symbol_map = {'up': '↑', 'down': '↓', 'left': '←', 'right': '→'}

def build_environment(grid_world, obstacles, goal_state):
    states = [(j, i) for j in range(grid_world[0]) for i in range(grid_world[1]) if (j, i) not in obstacles]
    assert goal_state in states, "Goal state cannot be an obstacle or out of bounds."
    goal_index = states.index(goal_state)
    return states, goal_index

def initialize_q_learning(states, actions):
    num_states, num_actions = len(states), len(actions)
    Q_values = np.zeros((num_states, num_actions))
    transition_probability = np.zeros((num_states, num_actions, num_states))
    reward_matrix = np.zeros((num_states, num_actions, num_states))
    return Q_values, transition_probability, reward_matrix

def build_model(states, grid_world, actions, obstacles, goal_index, transition_probability, reward_matrix):
    for i, s in enumerate(states):
        if i == goal_index:
            continue
        for j, action in enumerate(actions):
            move = action_moves[action]
            next_state = (s[0] + move[0], s[1] + move[1])
            next_state = (max(0, min(next_state[0], grid_world[0]-1)),
                          max(0, min(next_state[1], grid_world[1]-1)))
            if next_state in obstacles:
                next_state = s
            index_next_state = states.index(next_state)
            transition_probability[i, j, index_next_state] = 1.0
            reward_matrix[i, j, index_next_state] = GOAL_REWARD if index_next_state == goal_index else STEP_PENALTY

def train_q_learning(Q_values, transition_probability, reward_matrix, gamma=0.90, n_iterations=50):
    for _ in range(n_iterations):
        q_val = Q_values.copy()
        for s in range(Q_values.shape[0]):
            for a in range(Q_values.shape[1]):
                Q_values[s, a] = sum([
                    transition_probability[s, a, sp] * (
                        reward_matrix[s, a, sp] + gamma * np.max(q_val[sp])
                    )
                    for sp in range(Q_values.shape[0])
                ])
    return Q_values

def simulate_policy(Q_values, states, actions, start_index, goal_index):
    path = [states[start_index]]
    current_index = start_index

    while current_index != goal_index:
        best_action_index = np.argmax(Q_values[current_index])
        move = action_moves[actions[best_action_index]]
        current_state = states[current_index]
        next_state = (current_state[0] + move[0], current_state[1] + move[1])
        next_state = (max(0, min(next_state[0], grid_world[0]-1)),
                      max(0, min(next_state[1], grid_world[1]-1)))
        if next_state in OBSTACLES:
            next_state = current_state
        current_index = states.index(next_state)
        path.append(states[current_index])
    return path

def plot_policy_arrows(Q_values, grid_world, actions, states, goal_state, path=None, obstacles=None):
    symbols = [action_symbol_map[actions[np.argmax(row)]] for row in Q_values]
    symbols.insert(states.index(goal_state), '\U0001F3C6')  # Trophy for goal
    
    symbol_grid, idx = [], 0
    for i in range(grid_world[0]):
        row = []
        for j in range(grid_world[1]):
            if obstacles and (i, j) in obstacles:
                row.append('\u2588')
            elif (i, j) == goal_state:
                row.append('\U0001F3C6')
            else:
                row.append(symbols[idx])
                idx += 1
        symbol_grid.append(row)

    if path:
        for step_num, (i, j) in enumerate(path):
            if (i, j) == goal_state:
                continue
            if (i, j) == path[0]:
                symbol_grid[i][j] = "\u26F3"
            else:
                symbol_grid[i][j] = str(step_num % 10)

    print("Optimal Policy Grid:")
    for row in symbol_grid:
        print('    '.join(row))
    print()

def plot_grid(states, grid_world, obstacles, goal, path=None, Q_values=None, actions=None):
    fig, ax = plt.subplots()
    ax.set_xlim(0, grid_world[1])
    ax.set_ylim(0, grid_world[0])
    ax.set_xticks(range(grid_world[1]+1))
    ax.set_yticks(range(grid_world[0]+1))
    ax.set_aspect('equal')
    ax.invert_yaxis()

    for i in range(grid_world[0]):
        for j in range(grid_world[1]):
            rect = patches.Rectangle((j, i), 1, 1, linewidth=1, edgecolor='gray', facecolor='white')
            ax.add_patch(rect)
            if (i, j) in obstacles:
                rect.set_facecolor('black')
            elif (i, j) == goal:
                ax.text(j + 0.5, i + 0.5, "G:\u2605", ha='center', va='center', fontsize=20, color='red')

    if Q_values is not None:
        direction_map = {'up': (0, -0.3), 'down': (0, 0.3), 'left': (-0.3, 0), 'right': (0.3, 0)}
        for idx, state in enumerate(states):
            if state == goal:
                continue
            best_action = actions[np.argmax(Q_values[idx])]
            dx, dy = direction_map[best_action]
            x, y = state[1] + 0.5, state[0] + 0.5
            ax.arrow(x, y, dx, dy, head_width=0.1, head_length=0.1, fc='blue', ec='blue')

    if path:
        for idx, (i, j) in enumerate(path):
            if (i, j) != goal:
                ax.text(j + 0.5, i + 0.5, str(idx % 10), ha='center', va='center', fontsize=20, color='green')

    ax.grid(True)
    plt.show()


# Example execution block
if __name__ == "__main__":
    grid_world = [6, 6]
    OBSTACLES = [(1, 1), (2, 1), (3, 1), (4, 3), (4, 4), (1, 4), (2, 4), (3, 4)]

    GOAL_STATE = (5, 5)  # Change this to any valid coordinate not in OBSTACLES
    states, GOAL_INDEX = build_environment(grid_world, OBSTACLES, GOAL_STATE)

    actions = np.array(['up', 'down', 'left', 'right'])
    Q_values, T, R = initialize_q_learning(states, actions)
    build_model(states, grid_world, actions, OBSTACLES, GOAL_INDEX, T, R)
    Q_values = train_q_learning(Q_values, T, R)

    START_STATE = (2, 3)
    assert START_STATE not in OBSTACLES, "Start state is inside an obstacle!"
    START_INDEX = states.index(START_STATE)

    path = simulate_policy(Q_values, states, actions, START_INDEX, GOAL_INDEX)

    plot_policy_arrows(Q_values, grid_world, actions, states, GOAL_STATE, path=path, obstacles=OBSTACLES)
    plot_grid(states, grid_world, OBSTACLES, GOAL_STATE, path=path, Q_values=Q_values, actions=actions)
