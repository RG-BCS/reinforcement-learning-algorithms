# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# ---------------- CONFIGURATION ---------------- #
grid_world = [6, 6]
GOAL_REWARD = 10.0
STEP_PENALTY = -1.0
OBSTACLES = [(1, 1), (2, 1), (3, 1), (4, 3), (4, 4), (1, 4), (2, 4), (3, 4)]
GOAL_STATE = (5, 5)
START_STATE = (2, 3)

# ---------------- ENVIRONMENT SETUP ---------------- #
states = [(i, j) for i in range(grid_world[0]) for j in range(grid_world[1]) if (i, j) not in OBSTACLES]
GOAL_INDEX = states.index(GOAL_STATE)
START_INDEX = states.index(START_STATE)

actions = np.array(['up', 'down', 'left', 'right'])
action_moves = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}
action_symbol_map = {'up': '‚Üë', 'down': '‚Üì', 'left': '‚Üê', 'right': '‚Üí'}

transition_probability = np.zeros((len(states), len(actions), len(states)))
reward_matrix = np.zeros((len(states), len(actions), len(states)))
Q_values = np.zeros((len(states), len(actions)))

# Build transition and reward matrices
for i, s in enumerate(states):
    if s == GOAL_STATE:
        continue
    for j, a in enumerate(actions):
        move = action_moves[a]
        next_state = (s[0] + move[0], s[1] + move[1])
        next_state = (
            max(0, min(next_state[0], grid_world[0] - 1)),
            max(0, min(next_state[1], grid_world[1] - 1)),
        )
        if next_state in OBSTACLES:
            next_state = s
        index_next = states.index(next_state)
        transition_probability[i, j, index_next] = 1.0
        reward_matrix[i, j, index_next] = GOAL_REWARD if index_next == GOAL_INDEX else STEP_PENALTY

# ---------------- VALUE ITERATION ---------------- #
gamma = 0.90
for _ in range(50):
    prev_Q = Q_values.copy()
    for s in range(len(states)):
        for a in range(len(actions)):
            Q_values[s, a] = sum([
                transition_probability[s, a, sp] * (
                    reward_matrix[s, a, sp] + gamma * np.max(prev_Q[sp])
                )
                for sp in range(len(states))
            ])

# ---------------- UTILITIES ---------------- #
def simulate_policy(Q_values, states, actions, start_index, goal_index):
    path = [states[start_index]]
    current_index = start_index

    while current_index != goal_index:
        best_action = np.argmax(Q_values[current_index])
        move = action_moves[actions[best_action]]
        curr_state = states[current_index]
        next_state = (curr_state[0] + move[0], curr_state[1] + move[1])
        next_state = (
            max(0, min(next_state[0], grid_world[0] - 1)),
            max(0, min(next_state[1], grid_world[1] - 1)),
        )
        if next_state in OBSTACLES:
            next_state = curr_state
        current_index = states.index(next_state)
        path.append(states[current_index])
    return path

def plot_arrows(Q_values, grid_world, actions, states, goal_state, path=None, OBSTACLES=None):
    action_symbols = [action_symbol_map[action] for action in actions]
    policy_indices = np.argmax(Q_values, axis=1)
    policy_symbols = [action_symbols[i] for i in policy_indices]

    symbol_grid = []
    goal_idx = states.index(goal_state)
    policy_symbols[goal_idx] = '\U0001F3C6'  # üèÜ Trophy for goal

    idx = 0
    for i in range(grid_world[0]):
        row = []
        for j in range(grid_world[1]):
            if OBSTACLES and (i, j) in OBSTACLES:
                row.append('\u2588')
            elif (i, j) in states:
                row.append(policy_symbols[idx])
                idx += 1
            else:
                row.append(' ')
        symbol_grid.append(row)

    if path:
        for step_num, (i, j) in enumerate(path):
            if (i, j) == goal_state:
                continue
            elif (i, j) == path[0]:
                symbol_grid[i][j] = '\u26F3'  # ‚õ≥ Start
            else:
                symbol_grid[i][j] = str(step_num % 10)

    print("Optimal Policy Grid:")
    for row in symbol_grid:
        print("    ".join(row))
    print()

def plot_grid(states, grid_size, obstacles, goal, path=None, Q_values=None, actions=None):
    fig, ax = plt.subplots()
    ax.set_xlim(0, grid_size[1])
    ax.set_ylim(0, grid_size[0])
    ax.set_xticks(range(grid_size[1] + 1))
    ax.set_yticks(range(grid_size[0] + 1))
    ax.set_aspect('equal')
    ax.invert_yaxis()

    for i in range(grid_size[0]):
        for j in range(grid_size[1]):
            rect = patches.Rectangle((j, i), 1, 1, linewidth=1, edgecolor='gray', facecolor='white')
            ax.add_patch(rect)
            if (i, j) in obstacles:
                rect.set_facecolor('black')
            elif (i, j) == goal:
                ax.text(j+0.5, i+0.5, "G:\u2605", ha='center', va='center', fontsize=20, color='red')

    if Q_values is not None and actions is not None:
        action_to_vector = {'up': (0, -0.3), 'down': (0, 0.3), 'left': (-0.3, 0), 'right': (0.3, 0)}
        for idx, state in enumerate(states):
            if state == goal:
                continue
            best_action = actions[np.argmax(Q_values[idx])]
            dx, dy = action_to_vector[best_action]
            x, y = state[1] + 0.5, state[0] + 0.5
            ax.arrow(x, y, dx, dy, head_width=0.1, head_length=0.1, fc='blue', ec='blue')

    if path:
        for idx, (i, j) in enumerate(path):
            if (i, j) != goal:
                ax.text(j + 0.5, i + 0.5, str(idx % 10), ha='center', va='center', fontsize=20, color='green')

    ax.grid(True)
    plt.show()

# ---------------- RUN SCRIPT ---------------- #
if __name__ == "__main__":
    path = simulate_policy(Q_values, states, actions, START_INDEX, GOAL_INDEX)
    plot_arrows(Q_values, grid_world, actions, states, goal_state=GOAL_STATE, path=path, OBSTACLES=OBSTACLES)
    plot_grid(states, grid_world, OBSTACLES, GOAL_STATE, path=path, Q_values=Q_values, actions=actions)

