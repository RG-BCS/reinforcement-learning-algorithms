{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning in a 2×2 GridWorld\n",
        "\n",
        "This notebook demonstrates how Q-values are computed in a simple deterministic **2x2 gridworld** using a basic form of value iteration (offline Q-learning).\n",
        "\n",
        "It is designed to help build intuition for:\n",
        "- How states and actions interact in an MDP\n",
        "- How Q-values evolve over iterations\n",
        "- What an optimal policy looks like in grid navigation problems\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "-l5wWjjJbVoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and Definitions\n",
        "import numpy as np\n",
        "\n",
        "states = [(0, 0), (0, 1), (1, 0), (1, 1)]  # S0, S1, S2, S3\n",
        "actions = ['Up', 'Down', 'Left', 'Right']\n",
        "action_moves = {\n",
        "    'Up': (-1, 0),\n",
        "    'Down': (1, 0),\n",
        "    'Left': (0, -1),\n",
        "    'Right': (0, 1)\n",
        "}\n",
        "\n",
        "num_states = len(states)\n",
        "num_actions = len(actions)\n",
        "\n",
        "Q_values = np.zeros((num_states, num_actions))\n",
        "transition_prob = np.zeros((num_states, num_actions, num_states))\n",
        "rewards = np.zeros((num_states, num_actions, num_states))"
      ],
      "metadata": {
        "id": "DKhaWXCHbXMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Populate transition and reward matrices\n",
        "\n",
        "for i, state in enumerate(states):\n",
        "    for j, action in enumerate(actions):\n",
        "        move = action_moves[action]\n",
        "        next_state = (state[0] + move[0], state[1] + move[1])\n",
        "\n",
        "        # Clip next_state within grid boundaries (2x2 -> max index 1)\n",
        "        next_state = (\n",
        "            max(min(next_state[0], 1), 0),\n",
        "            max(min(next_state[1], 1), 0)\n",
        "        )\n",
        "\n",
        "        next_state_idx = states.index(next_state)\n",
        "        transition_prob[i, j, next_state_idx] = 1.0\n",
        "        rewards[i, j, next_state_idx] = 10 if next_state_idx == 3 else -1\n"
      ],
      "metadata": {
        "id": "K2Kk_ScYboXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Perform Q-value iteration\n",
        "\n",
        "gamma = 0.90\n",
        "iterations = 5\n",
        "\n",
        "for _ in range(iterations):\n",
        "    Q_old = Q_values.copy()\n",
        "    for s in range(num_states):\n",
        "        for a in range(num_actions):\n",
        "            Q_values[s, a] = sum([\n",
        "                transition_prob[s, a, sp] * (\n",
        "                    rewards[s, a, sp] + gamma * np.max(Q_old[sp])\n",
        "                )\n",
        "                for sp in range(num_states)\n",
        "            ])\n"
      ],
      "metadata": {
        "id": "fGKfFDkmbuMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: View final Q-values\n",
        "print(\"Final Q-Value Table:\")\n",
        "print(np.round(Q_values, 3))\n"
      ],
      "metadata": {
        "id": "-NcHbxEEbxtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Compute and visualize optimal policy\n",
        "\n",
        "action_symbols = ['↑', '↓', '←', '→']\n",
        "policy_indices = np.argmax(Q_values, axis=1)\n",
        "policy_symbols = [action_symbols[i] for i in policy_indices]\n",
        "\n",
        "print(\"\\nOptimal Policy Grid:\")\n",
        "print(f\"{policy_symbols[0]} {policy_symbols[1]}\")\n",
        "print(f\"{policy_symbols[2]} G\")  # G = goal state"
      ],
      "metadata": {
        "id": "rSaDyKjEb1PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Summary\n",
        "\n",
        "- Q-values converge after a few iterations.\n",
        "- The agent learns to navigate directly toward the goal at state **S3**.\n",
        "- This simple setup is a powerful tool to visualize and debug learning dynamics before generalizing to larger environments.\n",
        "\n",
        "Next steps:\n",
        "- Generalize to **N×N gridworlds**\n",
        "- Add **walls/obstacles**\n",
        "- Introduce **stochasticity** or explore other value functions (like S-values)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "9cfbruE8b5xc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f1_gquf7b9eV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}