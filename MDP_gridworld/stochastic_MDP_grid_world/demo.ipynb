{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic MDP Grid World Demo\n",
        "\n",
        "This notebook demonstrates the implementation and usage of a **stochastic Markov Decision Process (MDP)** in a grid world environment with obstacles and a goal state.\n",
        "\n",
        "We will:\n",
        "- Load the MDP environment\n",
        "- Visualize the optimal policy learned by value iteration\n",
        "- Simulate a rollout starting from a user-defined position\n",
        "- Visualize the policy and the path on the grid\n"
      ],
      "metadata": {
        "id": "lYNkrf7WJVeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import stochastic_mdp_grid_world as mdp  # Make sure this file is in your working directory"
      ],
      "metadata": {
        "id": "WJT16HWUJXe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid World Setup\n",
        "\n",
        "The grid world consists of a 5x6 grid with some obstacles and a goal state with a high reward.\n",
        "\n",
        "- Obstacles are cells that cannot be traversed.\n",
        "- The goal state gives a high reward.\n",
        "- Each step incurs a penalty.\n",
        "\n",
        "Let's look at the grid configuration:\n"
      ],
      "metadata": {
        "id": "CLZ0IoRoJc8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Grid World Size: {mdp.grid_world}\")\n",
        "print(f\"Obstacles at: {mdp.OBSTACLES}\")\n",
        "print(f\"Goal State: {mdp.GOAL_STATE}\")"
      ],
      "metadata": {
        "id": "ItEqwDtsJgSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimal Policy Computation\n",
        "\n",
        "The code uses value iteration to compute the optimal policy for navigating the grid world under stochastic dynamics.\n",
        "\n",
        "The policy is encoded in the Q-values matrix.\n",
        "\n",
        "Let's visualize the optimal policy using arrows that indicate the best action to take at each state.\n"
      ],
      "metadata": {
        "id": "QiSUw980Jneg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdp.plot_arrows_MDP_STOC(mdp.Q_values, mdp.grid_world, mdp.actions, mdp.states, mdp.GOAL_STATE, OBSTACLES=mdp.OBSTACLES)\n"
      ],
      "metadata": {
        "id": "QaO1Q1WaJrJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulate Policy Rollout\n",
        "\n",
        "We can simulate a deterministic rollout of the learned policy starting from any valid start state.\n",
        "\n",
        "Let's pick a start state and simulate the path taken to reach the goal state.\n"
      ],
      "metadata": {
        "id": "uXSr7sF5Jvpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a start state (not an obstacle)\n",
        "start_state = (4, 0)\n",
        "assert start_state not in mdp.OBSTACLES, \"Start state is inside an obstacle!\"\n",
        "\n",
        "start_index = mdp.states.index(start_state)\n",
        "path = mdp.simulate_policy_MDP_STOC(mdp.Q_values, mdp.states, mdp.actions, start_index, mdp.GOAL_INDEX)\n",
        "\n",
        "print(f\"Simulated path from {start_state} to goal:\")\n",
        "print(path)\n"
      ],
      "metadata": {
        "id": "dSjUTTTpJzjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Policy with Rollout Path\n",
        "\n",
        "Let's visualize the optimal policy again, but this time overlay the rollout path from the start state to the goal.\n",
        "\n",
        "The start state is marked with a flag, and each step in the path is shown with step numbers.\n"
      ],
      "metadata": {
        "id": "eMccEijeJ5gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdp.plot_arrows_MDP_STOC(mdp.Q_values, mdp.grid_world, mdp.actions, mdp.states, mdp.GOAL_STATE, OBSTACLES=mdp.OBSTACLES, path=path)\n"
      ],
      "metadata": {
        "id": "C-G1PpPYJ69F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Grid Visualization\n",
        "\n",
        "Finally, let's visualize the grid world including obstacles, goal, policy arrows, and the rollout path using matplotlib.\n",
        "\n",
        "This provides a clear and intuitive view of the environment and agent's policy.\n"
      ],
      "metadata": {
        "id": "XVhHRF3vKBMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdp.plot_grid_world_MDP_STOC(mdp.states, mdp.grid_world, mdp.OBSTACLES, mdp.GOAL_STATE, path=path, Q_values=mdp.Q_values, actions=mdp.actions)\n"
      ],
      "metadata": {
        "id": "Ca249SczKC4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "- We set up a stochastic grid world MDP with obstacles and a goal.\n",
        "- Used value iteration to compute optimal Q-values and policy.\n",
        "- Simulated a deterministic policy rollout from a given start state.\n",
        "- Visualized the policy and the rollout path both as text-based arrows and a graphical grid plot.\n",
        "\n",
        "This modular design makes it easy to experiment with different grid sizes, obstacles, rewards, and policies.\n"
      ],
      "metadata": {
        "id": "nBNlj_jCKF75"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RVl64sOEKJ1j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}