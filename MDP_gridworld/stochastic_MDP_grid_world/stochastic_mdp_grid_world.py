# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# Grid world dimensions (rows, columns)
grid_world = [5, 6]

# Rewards
GOAL_REWARD = 10.0
STEP_PENALTY = -1.0

# Obstacles with penalty reward
OBSTACLES = [(1, 4), (2, 3), (3, 4)]

# Define valid states excluding obstacles
states = []
for row in range(grid_world[0]):
    for col in range(grid_world[1]):
        if (row, col) not in OBSTACLES:
            states.append((row, col))

GOAL_STATE = (2, 4)  # Goal position
GOAL_INDEX = states.index(GOAL_STATE)

# Possible actions
actions = np.array(['up', 'down', 'left', 'right'])

# Action moves including possible veering (stochastic behavior)
action_moves = {
    'up': [(-1, 0), (0, -1), (0, 1)],
    'down': [(1, 0), (0, -1), (0, 1)],
    'left': [(0, -1), (-1, 0), (1, 0)],
    'right': [(0, 1), (-1, 0), (1, 0)]
}

# Map action names to symbols for display
action_symbol_map = {'up': '↑', 'down': '↓', 'left': '←', 'right': '→'}

# Transition probabilities for intended and veer moves
pre_defined_move_probas = {'intended': 0.8, 'veer_1': 0.1, 'veer_2': 0.1}

# Initialize transition and reward matrices
transition_probability = np.zeros((len(states), len(actions), len(states)))
reward_matrix = np.zeros((len(states), len(actions), len(states)))
Q_values = np.zeros((len(states), len(actions)))
actions_on_each_state = range(len(actions))


def validate_next_state(current_state, possible_moves, grid_size=grid_world, obstacle=OBSTACLES):
    """
    Validate and compute next possible states after taking moves from current state.
    Handles grid boundaries and obstacles by forcing agent to stay in place if move invalid.

    Parameters:
        current_state (tuple): (row, col) current agent position
        possible_moves (list): list of (row_delta, col_delta) moves
        grid_size (list): [rows, cols] size of grid
        obstacle (list): list of obstacle positions

    Returns:
        list of valid next states as (row, col) tuples
    """
    new_states = []
    for move in possible_moves:
        next_state = (current_state[0] + move[0], current_state[1] + move[1])
        # Clamp state within grid boundaries
        next_state_val = (max(min(next_state[0], grid_size[0] - 1), 0),
                          max(min(next_state[1], grid_size[1] - 1), 0))
        # If next state is an obstacle, stay in current state
        if next_state_val in obstacle:
            next_state_val = current_state
        new_states.append(next_state_val)
    return new_states


# Build transition and reward matrices
for i, state in enumerate(states):
    if i == GOAL_INDEX:
        continue  # No moves from goal state
    for j, action_name in enumerate(actions):
        intended_move, veer_1, veer_2 = validate_next_state(state, action_moves[action_name])
        intend_index = states.index(intended_move)
        veer_1_index = states.index(veer_1)
        veer_2_index = states.index(veer_2)
        collect_indices = {'intended': intend_index, 'veer_1': veer_1_index, 'veer_2': veer_2_index}

        # Assign transition probabilities
        transition_probability[i, j, intend_index] += pre_defined_move_probas['intended']
        transition_probability[i, j, veer_1_index] += pre_defined_move_probas['veer_1']
        transition_probability[i, j, veer_2_index] += pre_defined_move_probas['veer_2']

        # Assign rewards weighted by transition probabilities
        for move_type, idx in collect_indices.items():
            if idx == GOAL_INDEX:
                reward_matrix[i, j, idx] += GOAL_REWARD * pre_defined_move_probas[move_type]
            else:
                reward_matrix[i, j, idx] += STEP_PENALTY * pre_defined_move_probas[move_type]


def value_iteration(Q_values, transition_probability, reward_matrix, gamma=0.9, n_iterations=50):
    """
    Perform value iteration to compute optimal Q-values for all states and actions.

    Parameters:
        Q_values (np.array): initial Q-values, shape (states, actions)
        transition_probability (np.array): transition probabilities tensor (s,a,s')
        reward_matrix (np.array): reward matrix tensor (s,a,s')
        gamma (float): discount factor
        n_iterations (int): number of iterations

    Returns:
        np.array: optimized Q-values after iterations
    """
    for _ in range(n_iterations):
        q_val = Q_values.copy()
        for s in range(len(states)):
            for a in actions_on_each_state:
                Q_values[s, a] = sum(
                    transition_probability[s][a][sp] * (reward_matrix[s][a][sp] + gamma * np.max(q_val[sp]))
                    for sp in range(len(states))
                )
    return Q_values


def simulate_policy_MDP_STOC(Q_values, states, actions, start_index, goal_index):
    """
    Simulate a deterministic rollout of the policy derived from Q-values.

    Parameters:
        Q_values (np.array): optimal Q-values
        states (list): list of state tuples
        actions (np.array): array of action strings
        start_index (int): starting state index
        goal_index (int): goal state index

    Returns:
        list: sequence of states representing the path from start to goal
    """
    path = [states[start_index]]
    current_index = start_index
    current_state = path[0]

    while current_index != goal_index:
        best_action_index = np.argmax(Q_values[current_index])  # best action for current state
        move_increments = action_moves[actions[best_action_index]]  # action moves

        new_states = validate_next_state(current_state, move_increments)  # possible next states
        landing_states_index = [states.index(ns) for ns in new_states]

        # Pick next state with highest transition probability
        pick_max_trans_proba = np.array([
            transition_probability[current_index, best_action_index, idx] for idx in landing_states_index
        ])
        current_state = new_states[np.argmax(pick_max_trans_proba)]
        current_index = states.index(current_state)
        path.append(current_state)

    return path


def plot_arrows_MDP_STOC(Q_values, grid_world, actions, states, goal_state, path=None, OBSTACLES=None):
    """
    Print the optimal policy grid with arrows indicating best actions.
    Optionally overlay a path from start to goal.

    Parameters:
        Q_values (np.array): Q-values for policy extraction
        grid_world (list): [rows, cols]
        actions (np.array): list of action strings
        states (list): list of states
        goal_state (tuple): goal state coordinates
        path (list, optional): sequence of states representing a path
        OBSTACLES (list, optional): list of obstacle coordinates
    """
    action_symbol_map = {'up': '↑', 'down': '↓', 'left': '←', 'right': '→'}
    action_symbols = [action_symbol_map[a] for a in actions]
    policy_indices = np.argmax(Q_values, axis=1)
    policy_symbols = [action_symbols[i] for i in policy_indices]

    GOAL_STATE_SYMBOL = '\U0001F3C6'  # trophy emoji
    policy_symbols[states.index(goal_state)] = GOAL_STATE_SYMBOL

    symbol_grid = []
    OBSTACLE_SYMBOL = '\u2588'  # block
    START_STATE_SYMBOL = "\u26F3"  # flag

    idx = 0
    for i in range(grid_world[0]):
        row = []
        for j in range(grid_world[1]):
            if OBSTACLES and (i, j) in OBSTACLES:
                row.append(OBSTACLE_SYMBOL)
            else:
                row.append(policy_symbols[idx])
                idx += 1
        symbol_grid.append(row)

    # Overlay path steps if given
    STEP_MARK = '\u25CF'
    if path:
        for step_num, (i, j) in enumerate(path):
            if (i, j) == path[-1]:  # skip goal
                continue
            if (i, j) == path[0]:
                symbol_grid[i][j] = START_STATE_SYMBOL
            else:
                symbol_grid[i][j] = str(step_num % 10)

    print("Optimal Policy Grid:")
    for row in symbol_grid:
        print(''.join(f'{cell:^6}' for cell in row))
    print()


def plot_grid_world_MDP_STOC(states, grid_size, obstacles, goal, path=None, Q_values=None, actions=None):
    """
    Visualize the grid world, obstacles, goal, policy arrows, and agent path using matplotlib.

    Parameters:
        states (list): list of valid states
        grid_size (list): [rows, cols]
        obstacles (list): list of obstacle states
        goal (tuple): goal state coordinate
        path (list, optional): agent path to plot
        Q_values (np.array, optional): Q-values for policy arrows
        actions (np.array, optional): actions list
    """
    GOAL_STATE_SYMBOL = '\U0001F3C6'
    fig, ax = plt.subplots()
    ax.set_xlim(0, grid_size[1])
    ax.set_ylim(0, grid_size[0])
    ax.set_xticks(range(grid_size[1] + 1))
    ax.set_yticks(range(grid_size[0] + 1))
    ax.set_aspect('equal')
    ax.invert_yaxis()  # origin top-left

    # Draw grid and fill obstacles and goal
    for i in range(grid_size[0]):
        for j in range(grid_size[1]):
            rect = patches.Rectangle((j, i), 1, 1, linewidth=1, edgecolor='gray', facecolor='white')
            ax.add_patch(rect)
            if (i, j) in obstacles:
                rect.set_facecolor('black')
            elif (i, j) == goal:
                ax.text(j + 0.5, i + 0.5, "G:\u2605", ha='center', va='center', fontsize=20, color='red')

    # Draw policy arrows
    if Q_values is not None and actions is not None:
        action_to_vector = {
            'up': (0, -0.3),
            'down': (0, 0.3),
            'left': (-0.3, 0),
            'right': (0.3, 0)
        }
        for idx, state in enumerate(states):
            if state == goal:
                continue
            best_action = actions[np.argmax(Q_values[idx])]
            dx, dy = action_to_vector[best_action]
            x, y = state[1] + 0.5, state[0] + 0.5
            ax.arrow(x, y, dx, dy, head_width=0.1, head_length=0.1, fc='blue', ec='blue')

    # Draw agent path
    if path:
        for idx, (i, j) in enumerate(path):
            if (i, j) != goal:
                ax.text(j + 0.5, i + 0.5, str(idx % 10), ha='center', va='center', fontsize=20, color='green')

    ax.grid(True)
    plt.show()


if __name__ == "__main__":
    # Run value iteration
    Q_values = value_iteration(Q_values, transition_probability, reward_matrix)

    # Display optimal policy grid
    plot_arrows_MDP_STOC(Q_values, grid_world, actions, states, goal_state=GOAL_STATE, OBSTACLES=OBSTACLES)

    # Simulate path from a start position
    START_STATE = (4, 0)
    assert START_STATE not in OBSTACLES, "Start state is inside an obstacle!"
    START_INDEX = states.index(START_STATE)
    path = simulate_policy_MDP_STOC(Q_values, states, actions, START_INDEX, GOAL_INDEX)

    # Display policy grid with path overlay
    plot_arrows_MDP_STOC(Q_values, grid_world, actions, states, goal_state=GOAL_STATE, OBSTACLES=OBSTACLES, path=path)

    # Plot visual grid world with path and policy arrows
    plot_grid_world_MDP_STOC(states, grid_size=grid_world, obstacles=OBSTACLES,
                             goal=GOAL_STATE, path=path, Q_values=Q_values, actions=actions)

