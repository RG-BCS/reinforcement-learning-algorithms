# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

# demo_script.py

import stochastic_mdp_grid_world as mdp
import numpy as np

def main():
    # Just re-run value iteration to be sure (optional, since it's done in your module)
    # You can skip this if you run the module directly and have the Q_values ready

    # Plot the optimal policy without path
    mdp.plot_arrows_MDP_STOC(mdp.Q_values, mdp.grid_world, mdp.actions, mdp.states, mdp.GOAL_STATE, OBSTACLES=mdp.OBSTACLES)

    # Define a start state for the simulation
    START_STATE = (4, 0)
    if START_STATE in mdp.OBSTACLES:
        raise ValueError("Start state is inside an obstacle!")

    START_INDEX = mdp.states.index(START_STATE)

    # Simulate policy rollout from start state to goal
    path = mdp.simulate_policy_MDP_STOC(mdp.Q_values, mdp.states, mdp.actions, START_INDEX, mdp.GOAL_INDEX)

    # Plot the policy with the rollout path
    mdp.plot_arrows_MDP_STOC(mdp.Q_values, mdp.grid_world, mdp.actions, mdp.states, mdp.GOAL_STATE, OBSTACLES=mdp.OBSTACLES, path=path)

    # Plot full grid with policy arrows and path
    mdp.plot_grid_world_MDP_STOC(mdp.states, mdp.grid_world, mdp.OBSTACLES, mdp.GOAL_STATE, path=path, Q_values=mdp.Q_values, actions=mdp.actions)

if __name__ == "__main__":
    main()