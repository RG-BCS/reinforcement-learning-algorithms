{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prioritized Experience Replay Double DQN on LunarLander-v2\n",
        "\n",
        "This notebook demonstrates training a Double Deep Q-Network (Double DQN) with Prioritized Experience Replay (PER) from scratch on the LunarLander-v2 environment from OpenAI Gym.\n",
        "\n",
        "We'll cover:\n",
        "- Environment setup and seeding for reproducibility\n",
        "- Neural network architecture for Q-learning\n",
        "- Prioritized Experience Replay buffer implementation\n",
        "- Training loop with epsilon-greedy exploration\n",
        "- Evaluation of the trained agent\n",
        "- Visualization of training progress (episode rewards)\n"
      ],
      "metadata": {
        "id": "jBzWPBnkCnTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic imports\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "SEED = 43\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n"
      ],
      "metadata": {
        "id": "rfztGUnuCo7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Q-network\n",
        "\n",
        "A simple fully-connected neural network that maps states to Q-values for each action.\n"
      ],
      "metadata": {
        "id": "2-cj-Z7mCuGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QT_Network(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.policy_model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.policy_model(x)\n"
      ],
      "metadata": {
        "id": "neEXeKdzCtgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Prioritized Replay Buffer class\n",
        "\n",
        "This buffer stores transitions along with their priorities (TD-errors) for prioritized sampling.\n"
      ],
      "metadata": {
        "id": "wQIQhmYDC0BC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity, alpha=0.6):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.priorities = []\n",
        "        self.alpha = alpha\n",
        "        self.pos = 0\n",
        "\n",
        "    def add(self, transition, td_error=1.0):\n",
        "        priority = (abs(td_error) + 1e-5) ** self.alpha\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(transition)\n",
        "            self.priorities.append(priority)\n",
        "        else:\n",
        "            self.buffer[self.pos] = transition\n",
        "            self.priorities[self.pos] = priority\n",
        "        self.pos = (self.pos + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        priorities = np.array(self.priorities)\n",
        "        probs = priorities / priorities.sum()\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
        "        samples = [self.buffer[i] for i in indices]\n",
        "        total = len(self.buffer)\n",
        "        weights = (total * probs[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "        return samples, indices, torch.FloatTensor(weights).unsqueeze(1)\n",
        "\n",
        "    def update_priorities(self, indices, td_errors):\n",
        "        for idx, td_err in zip(indices, td_errors):\n",
        "            self.priorities[idx] = (abs(td_err.item()) + 1e-5) ** self.alpha\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "lhbSUCLNC3aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters and Environment Setup\n"
      ],
      "metadata": {
        "id": "GLisWHz7C6-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "input_dim = env.observation_space.shape[0]\n",
        "output_dim = env.action_space.n\n",
        "max_steps = env.spec.max_episode_steps\n",
        "\n",
        "# Hyperparameters\n",
        "replay_max = 10000\n",
        "learning_rate = 1e-3\n",
        "n_episodes = 500\n",
        "epsilon_min, epsilon_decay = 0.01, 0.995\n",
        "batch_size = 64\n",
        "discounted_factor = 0.99\n",
        "beta = 0.4\n",
        "alpha = 0.6\n",
        "target_update_freq = 10  # episodes\n"
      ],
      "metadata": {
        "id": "capEY4fwC-R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize replay buffer, networks, loss function, optimizer\n"
      ],
      "metadata": {
        "id": "SBAH7QuaDBsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = PrioritizedReplayBuffer(capacity=replay_max, alpha=alpha)\n",
        "Q_network = QT_Network(input_dim, output_dim)\n",
        "target_network = QT_Network(input_dim, output_dim)\n",
        "target_network.load_state_dict(Q_network.state_dict())\n",
        "target_network.eval()\n",
        "loss_fn = nn.SmoothL1Loss()\n",
        "optimizer = optim.Adam(Q_network.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "McSYO5BnDBYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop with Epsilon-Greedy Exploration\n",
        "\n",
        "We sample batches using prioritized replay, compute TD errors, update priorities, and periodically sync the target network.\n"
      ],
      "metadata": {
        "id": "7B9XcIgsDIub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 1.0\n",
        "rewards_history = []\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    if isinstance(state, tuple):\n",
        "        state = state[0]\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        if random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_vals = Q_network(state_tensor)\n",
        "                action = torch.argmax(q_vals, dim=1).item()\n",
        "\n",
        "        result = env.step(action)\n",
        "        if len(result) == 5:\n",
        "            next_state, reward, terminated, truncated, _ = result\n",
        "            done = terminated or truncated\n",
        "        else:\n",
        "            next_state, reward, done, _ = result\n",
        "\n",
        "        replay_buffer.add((state, action, reward, next_state, done))\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "        if len(replay_buffer) >= batch_size:\n",
        "            batch, indices, weights = replay_buffer.sample(batch_size, beta=beta)\n",
        "            states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "            states = torch.FloatTensor(np.array(states))\n",
        "            rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
        "            actions = torch.LongTensor(actions).unsqueeze(1)\n",
        "            next_states = torch.FloatTensor(np.array(next_states))\n",
        "            dones = torch.FloatTensor(dones).unsqueeze(1)\n",
        "\n",
        "            q_values = Q_network(states).gather(1, actions)\n",
        "            with torch.no_grad():\n",
        "                next_actions = Q_network(next_states).argmax(1, keepdim=True)\n",
        "                next_q_values = target_network(next_states).gather(1, next_actions)\n",
        "                target_q_values = rewards + (1 - dones) * discounted_factor * next_q_values\n",
        "\n",
        "            td_errors = (target_q_values - q_values).detach()\n",
        "            loss = (weights * loss_fn(q_values, target_q_values)).mean()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(Q_network.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            replay_buffer.update_priorities(indices, torch.abs(td_errors))\n",
        "\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    if episode % target_update_freq == 0:\n",
        "        target_network.load_state_dict(Q_network.state_dict())\n",
        "\n",
        "    rewards_history.append(total_reward)\n",
        "\n",
        "    if episode % 50 == 0:\n",
        "        print(f\"Episode {episode} - Total Reward: {total_reward:.2f} - Epsilon: {epsilon:.3f}\")\n"
      ],
      "metadata": {
        "id": "rKSTZuh_DKCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting Training Rewards\n",
        "\n",
        "Let's visualize how total episode rewards improve over time.\n"
      ],
      "metadata": {
        "id": "H3sIxcFdDRqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(rewards_history)\n",
        "plt.title(\"Episode Rewards Over Training\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dgDaXOLfDRRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Function\n",
        "\n",
        "Test the trained agent's performance over several episodes without exploration.\n"
      ],
      "metadata": {
        "id": "tmcw2PnTDUf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, model, episodes=10, max_steps=1000, render=False):\n",
        "    model.eval()\n",
        "    total_rewards = []\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        if isinstance(state, tuple):\n",
        "            state = state[0]\n",
        "\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                action = torch.argmax(model(state_tensor)).item()\n",
        "\n",
        "            result = env.step(action)\n",
        "            if len(result) == 5:\n",
        "                next_state, reward, terminated, truncated, _ = result\n",
        "                done = terminated or truncated\n",
        "            else:\n",
        "                next_state, reward, done, _ = result\n",
        "\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        total_rewards.append(episode_reward)\n",
        "\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    print(f\"Average reward over {episodes} episodes: {avg_reward:.2f}\")\n",
        "    return avg_reward\n"
      ],
      "metadata": {
        "id": "t0iu-zMQDXu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the trained model\n"
      ],
      "metadata": {
        "id": "4LbjkH1eDcOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_agent(env, Q_network, episodes=10, render=True)\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "gBGgzbIHDb3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "- We successfully trained a Double DQN agent with Prioritized Experience Replay on LunarLander-v2.\n",
        "- The agent learns efficient policies faster by sampling important experiences more often.\n",
        "- The reward plot shows steady improvement and stable convergence.\n",
        "- This demonstrates the power of combining PER with Double DQN and target networks.\n",
        "\n",
        "Feel free to experiment with hyperparameters or network architectures to improve performance further!\n"
      ],
      "metadata": {
        "id": "_wNAMFGCDi3s"
      }
    }
  ]
}