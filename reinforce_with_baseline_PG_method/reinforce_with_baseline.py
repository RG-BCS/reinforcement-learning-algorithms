# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

import gym
import numpy as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings("ignore", category=DeprecationWarning)

# Set random seeds for reproducibility
seed = 65
np.random.seed(seed)
tf.random.set_seed(seed)

def play_game(env, policy_model, gamma=0.99, epsilon=1e-9):
    """
    Play one full episode using the current policy model.

    Returns:
        states (np.ndarray): Observed states during the episode.
        actions (np.ndarray): Actions taken (0 or 1).
        norm_rewards (np.ndarray): Normalized discounted returns (used as advantage).
        rewards (list): Raw rewards from the environment.
    """
    states, actions, rewards = [], [], []
    obs = env.reset()
    done = False

    while not done:
        states.append(obs)
        # Stochastic action sampling from policy network
        p_left = policy_model(obs.reshape(1, -1), training=False)[0, 0].numpy()
        action = np.random.rand() > p_left  # Bernoulli sampling: 0=left, 1=right
        obs, reward, done, _ = env.step(int(action))
        actions.append(action)
        rewards.append(reward)

    # Compute discounted returns
    G = 0
    disc_rewards = []
    for r in reversed(rewards):
        G = r + gamma * G
        disc_rewards.insert(0, G)
    disc_rewards = np.array(disc_rewards)

    # Normalize returns to use as advantage
    norm_rewards = (disc_rewards - disc_rewards.mean()) / (disc_rewards.std() + epsilon)
    return np.array(states), np.array(actions), norm_rewards, rewards

def evaluate_episode(model, env, max_steps=500):
    """
    Evaluate current policy by running one episode in the environment.
    Returns total accumulated reward.
    """
    obs = env.reset()
    total_reward = 0
    steps = 0
    done = False

    while not done and steps < max_steps:
        p_left = model(obs.reshape(1, -1), training=False)[0, 0].numpy()
        action = np.random.rand() > p_left
        obs, reward, done, _ = env.step(int(action))
        total_reward += reward
        steps += 1

    return total_reward

def train_policy_model(n_episodes, env_name, policy_model, gamma=0.99, epsilon=1e-9, log_interval=50):
    """
    Train a stochastic policy using REINFORCE with baseline (average return).
    """
    reward_history = []
    env = gym.make(env_name)
    env.seed(seed)
    env.action_space.seed(seed)

    optimizer = keras.optimizers.Adam(learning_rate=0.01)

    for episode in range(n_episodes):
        with tf.GradientTape() as tape:
            # Generate trajectory
            states, actions, norm_rewards, raw_rewards = play_game(env, policy_model, gamma, epsilon)

            states = tf.convert_to_tensor(states, dtype=tf.float32)
            actions = tf.convert_to_tensor(actions, dtype=tf.float32)
            norm_rewards = tf.convert_to_tensor(norm_rewards, dtype=tf.float32)

            # Compute action probabilities
            p_left = tf.reshape(policy_model(states), (-1,))
            action_probs = tf.where(actions == 1, p_left, 1 - p_left)
            action_probs = tf.clip_by_value(action_probs, 1e-8, 1.0)

            log_probs = tf.math.log(action_probs + epsilon)

            # Baseline: average return across episode
            advantage = norm_rewards - tf.reduce_mean(norm_rewards)

            # Policy gradient loss
            loss = -tf.reduce_mean(log_probs * advantage)

        # Gradient update
        grads = tape.gradient(loss, policy_model.trainable_variables)
        optimizer.apply_gradients(zip(grads, policy_model.trainable_variables))

        reward_history.append(sum(raw_rewards))

        if episode % log_interval == 0:
            print(f"Episode {episode}, Reward: {sum(raw_rewards)}, Loss: {loss.numpy():.4f}")

    return reward_history

def main():
    env_name = "CartPole-v1"
    n_inputs = 4
    n_episodes = 500
    gamma = 0.99

    # Define the policy network
    policy_model = keras.Sequential([
        keras.layers.InputLayer(shape=(n_inputs,)),
        keras.layers.Dense(5, activation='elu'),
        keras.layers.Dense(1, activation='sigmoid')
    ])

    test_env = gym.make(env_name)
    print(f"Test before training: total reward = {evaluate_episode(policy_model, test_env)}")

    # Train policy
    rewards = train_policy_model(n_episodes, env_name, policy_model, gamma)

    print(f"\nTest after training: total reward = {evaluate_episode(policy_model, test_env)}")

    # Average test performance over 20 runs
    avg_test = np.mean([evaluate_episode(policy_model, test_env) for _ in range(20)])
    print(f"Average reward over 20 test episodes: {avg_test:.2f}")

    # Plot reward curve
    plt.plot(rewards)
    plt.xlabel("Episode")
    plt.ylabel("Total Reward")
    plt.title("REINFORCE with Baseline â€” Training Progress")
    plt.grid(True)
    plt.show()

if __name__ == "__main__":
    main()