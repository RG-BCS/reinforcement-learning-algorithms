{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# REINFORCE with Baseline — Policy Gradient on CartPole-v1\n",
        "\n",
        "This notebook demonstrates the use of the REINFORCE algorithm with a baseline to train a neural network policy for the CartPole-v1 environment. The baseline helps reduce the variance of the policy gradient updates.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "fL4oPCjSHiQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and setup\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "seed = 65\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "Nuj6nfIaHvE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Setup\n",
        "\n",
        "We start by building a policy network that outputs probabilities for taking an action, and test how it performs without training."
      ],
      "metadata": {
        "id": "QU9zjJKNH2G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create environment and untrained policy model\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "n_inputs = 4\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.InputLayer(shape=(n_inputs,)),\n",
        "    keras.layers.Dense(5, activation=\"elu\"),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# Evaluate untrained model\n",
        "def evaluate_episode(model, env, max_steps=500):\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    while not done and steps < max_steps:\n",
        "        p_left = model(obs.reshape(1, -1), training=False)[0, 0].numpy()\n",
        "        action = np.random.rand() > p_left\n",
        "        obs, reward, done, _ = env.step(int(action))\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "    return total_reward\n",
        "\n",
        "print(f\"Initial reward before training: {evaluate_episode(model, env)}\")\n"
      ],
      "metadata": {
        "id": "wonAJrZ5HzS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Function to Play One Game\n",
        "\n",
        "We'll play full episodes, collecting states, actions, and rewards. Then we compute the discounted and normalized returns."
      ],
      "metadata": {
        "id": "Unt5J-6DICek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_game(env, policy_model, gamma=0.99, epsilon=1e-9):\n",
        "    states, actions, rewards = [], [], []\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        states.append(obs)\n",
        "        p_left = policy_model(obs.reshape(1, -1), training=False)[0, 0].numpy()\n",
        "        action = np.random.rand() > p_left\n",
        "        obs, reward, done, _ = env.step(int(action))\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    # Compute discounted return\n",
        "    disc_rewards = []\n",
        "    G = 0\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        disc_rewards.insert(0, G)\n",
        "    disc_rewards = np.array(disc_rewards)\n",
        "\n",
        "    # Normalize\n",
        "    norm_rewards = (disc_rewards - disc_rewards.mean()) / (disc_rewards.std() + epsilon)\n",
        "    return np.array(states), np.array(actions), norm_rewards, rewards"
      ],
      "metadata": {
        "id": "eCJK3MO1IDrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Policy\n",
        "\n",
        "We train the network using policy gradients and a baseline — the baseline in this case is the average return of the episode.\n"
      ],
      "metadata": {
        "id": "f1LLwT_nIHwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_policy_model(n_episodes, env_name, policy_model, gamma=0.99, epsilon=1e-9):\n",
        "    reward_history = []\n",
        "    env = gym.make(env_name)\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        with tf.GradientTape() as tape:\n",
        "            states, actions, norm_rewards, rewards = play_game(env, policy_model, gamma, epsilon)\n",
        "\n",
        "            states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "            actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
        "            norm_rewards = tf.convert_to_tensor(norm_rewards, dtype=tf.float32)\n",
        "\n",
        "            p_left = tf.reshape(policy_model(states), (-1,))\n",
        "            action_probs = tf.where(actions == 0, p_left, 1 - p_left)\n",
        "            log_probs = tf.math.log(tf.clip_by_value(action_probs, 1e-8, 1.0))\n",
        "\n",
        "            # Baseline as average reward\n",
        "            advantage = norm_rewards - tf.reduce_mean(norm_rewards)\n",
        "            loss = -tf.reduce_mean(log_probs * advantage)\n",
        "\n",
        "        grads = tape.gradient(loss, policy_model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, policy_model.trainable_variables))\n",
        "\n",
        "        reward_history.append(sum(rewards))\n",
        "        if episode % 50 == 0:\n",
        "            print(f\"Episode {episode}, Reward: {sum(rewards)}, Loss: {loss.numpy():.4f}\")\n",
        "    return reward_history"
      ],
      "metadata": {
        "id": "ycpjXVAIILGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Agent\n"
      ],
      "metadata": {
        "id": "VxprcjiZIPwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_history = train_policy_model(n_episodes=500,env_name=\"CartPole-v1\",\n",
        "                                    policy_model=model,gamma=0.99)\n"
      ],
      "metadata": {
        "id": "ZcDe3eGRISYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-Training Evaluation\n"
      ],
      "metadata": {
        "id": "lZLHvEQ1Ieut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_reward = evaluate_episode(model, env)\n",
        "print(f\"Total reward after training: {final_reward}\")\n",
        "\n",
        "test_runs = 20\n",
        "avg_reward = np.mean([evaluate_episode(model, env) for _ in range(test_runs)])\n",
        "print(f\"Average reward over {test_runs} episodes: {avg_reward:.2f}\")\n"
      ],
      "metadata": {
        "id": "loNqVY1sIeYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reward Curve"
      ],
      "metadata": {
        "id": "8_IU2OsxIlpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(reward_history)\n",
        "plt.title(\"Training Progress — REINFORCE with Baseline\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZBAHRndTInaK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
